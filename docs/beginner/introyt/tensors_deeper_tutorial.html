


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ko" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ko" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta property="og:title" content="Pytorch Tensor 소개" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/beginner/introyt/tensors_deeper_tutorial.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="Introduction|| Tensors|| Autograd|| Building Models|| TensorBoard Support|| Training Models|| Model Understanding 번역: 이상윤 아래 영상이나 youtube 를 참고하세요. Tensor는 PyTorch에서 중요한 추상 데이터 자료형입니다. 이 interactive notebook은 torch.Tensor 클래스에 대한 심층적인 소개를 제공합니다. 먼저 가장 중요한 것은 PyTorch 모듈을 import 하는 것입니다. 또한 몇 가지 예제에..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="Introduction|| Tensors|| Autograd|| Building Models|| TensorBoard Support|| Training Models|| Model Understanding 번역: 이상윤 아래 영상이나 youtube 를 참고하세요. Tensor는 PyTorch에서 중요한 추상 데이터 자료형입니다. 이 interactive notebook은 torch.Tensor 클래스에 대한 심층적인 소개를 제공합니다. 먼저 가장 중요한 것은 PyTorch 모듈을 import 하는 것입니다. 또한 몇 가지 예제에..." />
<meta property="og:ignore_canonical" content="true" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Pytorch Tensor 소개 &mdash; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom2.css" type="text/css" />
    <link rel="index" title="색인" href="../../genindex.html" />
    <link rel="search" title="검색" href="../../search.html" />
    <link rel="next" title="The Fundamentals of Autograd" href="autogradyt_tutorial.html" />
    <link rel="prev" title="PyTorch 소개" href="introyt1_tutorial.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.kr/get-started">시작하기</a>
          </li>

          <li class="active">
            <a href="https://tutorials.pytorch.kr/">튜토리얼</a>
          </li>

          <li>
            <a href="https://pytorch.kr/hub">허브</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">커뮤니티</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  2.3.1
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">파이토치(PyTorch) 레시피</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../recipes/recipes_index.html">모든 레시피 보기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../prototype/prototype_index.html">모든 프로토타입 레시피 보기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">파이토치(PyTorch) 시작하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basics/intro.html">파이토치(PyTorch) 기본 익히기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/quickstart_tutorial.html">빠른 시작(Quickstart)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/tensorqs_tutorial.html">텐서(Tensor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/data_tutorial.html">Dataset과 DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/transforms_tutorial.html">변형(Transform)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/buildmodel_tutorial.html">신경망 모델 구성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/autogradqs_tutorial.html"><code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code>를 사용한 자동 미분</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/optimization_tutorial.html">모델 매개변수 최적화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics/saveloadrun_tutorial.html">모델 저장하고 불러오기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch on YouTube</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introyt.html">PyTorch 소개 - YouTube 시리즈</a></li>
<li class="toctree-l1"><a class="reference internal" href="introyt1_tutorial.html">PyTorch 소개</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Pytorch Tensor 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboardyt_tutorial.html">PyTorch TensorBoard 지원</a></li>
<li class="toctree-l1"><a class="reference internal" href="trainingyt.html">Training with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="captumyt.html">Model Understanding with Captum</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">파이토치(PyTorch) 배우기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning_60min_blitz.html">PyTorch로 딥러닝하기: 60분만에 끝장내기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch_with_examples.html">예제로 배우는 파이토치(PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nn_tutorial.html"><cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tensorboard_tutorial.html">TensorBoard로 모델, 데이터, 학습 시각화하기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">이미지/비디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../transfer_learning_tutorial.html">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fgsm_tutorial.html">적대적 예제 생성(Adversarial Example Generation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dcgan_faces_tutorial.html">DCGAN 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vt_tutorial.html">배포를 위해 비전 트랜스포머(Vision Transformer) 모델 최적화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tiatoolbox_tutorial.html">Pytorch와 TIAToolbox를 사용한 전체 슬라이드 이미지(Whole Slide Image) 분류</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">오디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../audio_io_tutorial.html">Audio I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../audio_resampling_tutorial.html">Audio Resampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../audio_data_augmentation_tutorial.html">Audio Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../audio_feature_extractions_tutorial.html">Audio Feature Extractions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../audio_feature_augmentation_tutorial.html">Audio Feature Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../audio_datasets_tutorial.html">오디오 데이터셋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/speech_recognition_pipeline_tutorial.html">Speech Recognition with Wav2Vec2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/text_to_speech_with_torchaudio.html">Text-to-speech with Tacotron2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/forced_alignment_with_torchaudio_tutorial.html">wav2vec2을 이용한 강제 정렬</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">텍스트</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../bettertransformer_tutorial.html">Better Transformer를 이용한 고속 트랜스포머 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html">기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html">기초부터 시작하는 NLP:  문자-단위 RNN으로 이름 생성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html">기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역</a></li>
<li class="toctree-l1"><a class="reference internal" href="../text_sentiment_ngrams_tutorial.html">torchtext 라이브러리로 텍스트 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../translation_transformer.html"><code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> 와 torchtext로 언어 번역하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../torchtext_custom_dataset_tutorial.html">TorchText를 사용하여 사용자 정의 글 데이터셋 전처리하기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">백엔드</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../onnx/intro_onnx.html">Introduction to ONNX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">강화학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html">강화 학습 (DQN) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/mario_rl_tutorial.html">마리오 게임 RL 에이전트로 학습하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch 모델을 프로덕션 환경에 배포하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../onnx/intro_onnx.html">Introduction to ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/flask_rest_api_tutorial.html">Flask를 사용하여 Python에서 PyTorch를 REST API로 배포하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Intro_to_TorchScript_tutorial.html">TorchScript 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_export.html">C++에서 TorchScript 모델 로딩하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/super_resolution_with_onnxruntime.html">(선택) PyTorch 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/realtime_rpi.html">Raspberry Pi 4 에서 실시간 추론(Inference) (30fps!)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch 프로파일링</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../profiler.html">PyTorch 모듈 프로파일링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hta_intro_tutorial.html">전체론적(Holistic Trace Analysis) 추적 분석 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hta_trace_diff_tutorial.html">종합적 분석을 이용한 트레이스 차이 분석</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/fx_conv_bn_fuser.html">(베타) FX에서 합성곱/배치 정규화(Convolution/Batch Norm) 결합기(Fuser) 만들기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">프론트엔드 API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/memory_format_tutorial.html">(베타) PyTorch를 사용한 Channels Last 메모리 형식</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ensembling.html">모델 앙상블</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_frontend.html">PyTorch C++ 프론트엔드 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch-script-parallelism.html">TorchScript의 동적 병렬 처리(Dynamic Parallelism)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_autograd.html">C++ 프론트엔드의 자동 미분 (autograd)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch 확장하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/custom_function_double_backward_tutorial.html">Double Backward with Custom Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/custom_function_conv_bn_tutorial.html">Fusing Convolution and Batch Norm using Custom Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch_script_custom_classes.html">커스텀 C++ 클래스로 TorchScript 확장하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/dispatcher.html">Registering a Dispatched Operator in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/extend_dispatcher.html">Extending dispatcher for a new backend in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/privateuseone.html">Facilitating New Backend Integration by PrivateUse1</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">모델 최적화</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../profiler.html">PyTorch 모듈 프로파일링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tensorboard_profiler_tutorial.html">텐서보드를 이용한 파이토치 프로파일러</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hyperparameter_tuning_tutorial.html">Ray Tune을 사용한 하이퍼파라미터 튜닝</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vt_tutorial.html">배포를 위해 비전 트랜스포머(Vision Transformer) 모델 최적화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/pruning_tutorial.html">가지치기 기법(Pruning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/dynamic_quantization_tutorial.html">(베타) LSTM 기반 단어 단위 언어 모델의 동적 양자화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dynamic_quantization_bert_tutorial.html">(베타) BERT 모델 동적 양자화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/quantized_transfer_learning_tutorial.html">(베타) 컴퓨터 비전 튜토리얼을 위한 양자화된 전이학습(Quantized Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/static_quantization_tutorial.html">(베타) PyTorch에서 Eager Mode를 이용한 정적 양자화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchserve_with_ipex.html">Grokking PyTorch Intel CPU performance from first principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchserve_with_ipex_2.html">Grokking PyTorch Intel CPU performance from first principles (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/nvfuser_intro_tutorial.html">Getting Started - Accelerate Your Scripts with nvFuser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torch_compile_tutorial.html">Introduction to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html">(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#torch-compile-sdpa"><code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> 과 함께 SDPA 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#sdpa-atteition-bias">SDPA를 <code class="docutils literal notranslate"><span class="pre">atteition.bias</span></code> 하위 클래스와 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#id8">결론</a></li>
<li class="toctree-l1"><a class="reference internal" href="../knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">병렬 및 분산 학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../distributed/home.html">Distributed and Parallel Training Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ddp_series_intro.html">PyTorch의 분산 데이터 병렬 처리 - 비디오 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/model_parallel_tutorial.html">단일 머신을 사용한 모델 병렬화 모범 사례</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ddp_tutorial.html">분산 데이터 병렬 처리 시작하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dist_tuto.html">PyTorch로 분산 어플리케이션 개발하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/FSDP_tutorial.html">Fully Sharded Data Parallel(FSDP) 시작하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/FSDP_adavnced_tutorial.html">Advanced Model Training with Fully Sharded Data Parallel (FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/TP_tutorial.html">Large Scale Transformer model training with Tensor Parallel (TP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/process_group_cpp_extension_tutorial.html">Cpp 확장을 사용한 프로세스 그룹 백엔드 사용자 정의</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/rpc_ddp_tutorial.html">분산 데이터 병렬(DDP)과 분산 RPC 프레임워크 결합</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/ddp_pipeline.html">분산 데이터 병렬 처리와 병렬 처리 파이프라인을 사용한 트랜스포머 모델 학습</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/generic_join.html">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Edge with ExecuTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/tutorials/export-to-executorch-tutorial.html">Exporting to ExecuTorch Tutorial</a></li>
<li class="toctree-l1"><a class="reference external" href=" https://pytorch.org/executorch/stable/running-a-model-cpp-tutorial.html">Running an ExecuTorch Model in C++ Tutorial</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/tutorials/sdk-integration-tutorial.html">Using the ExecuTorch SDK to Profile a Model</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/demo-apps-ios.html">Building an ExecuTorch iOS Demo App</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/demo-apps-android.html">Building an ExecuTorch Android Demo App</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/examples-end-to-end-to-lower-model-to-delegate.html">Lowering a Model as a Delegate</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">추천 시스템</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchrec_tutorial.html">TorchRec 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multimodality</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../flava_finetuning_tutorial.html">TorchMultimodal 튜토리얼: FLAVA 미세조정</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
      <li>Pytorch Tensor 소개</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../_sources/beginner/introyt/tensors_deeper_tutorial.rst.txt" rel="nofollow"><img src="../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">beginner/introyt/tensors_deeper_tutorial</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        

          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-beginner-introyt-tensors-deeper-tutorial-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<p class="sphx-glr-example-title" id="sphx-glr-beginner-introyt-tensors-deeper-tutorial-py"><a class="reference external" href="introyt1_tutorial.html">Introduction</a> ||
<strong>Tensors</strong> ||
<a class="reference external" href="autogradyt_tutorial.html">Autograd</a> ||
<a class="reference external" href="modelsyt_tutorial.html">Building Models</a> ||
<a class="reference external" href="tensorboardyt_tutorial.html">TensorBoard Support</a> ||
<a class="reference external" href="trainingyt.html">Training Models</a> ||
<a class="reference external" href="captumyt.html">Model Understanding</a></p>
<div class="section" id="pytorch-tensor">
<h1>Pytorch Tensor 소개<a class="headerlink" href="#pytorch-tensor" title="이 제목에 대한 퍼머링크">¶</a></h1>
<p>번역: <a class="reference external" href="https://github.com/falconlee236">이상윤</a></p>
<p>아래 영상이나 <a class="reference external" href="https://www.youtube.com/watch?v=r7QDUPb2dCM">youtube</a> 를 참고하세요.</p>
<div style="margin-top:10px; margin-bottom:10px;">
  <iframe width="560" height="315" src="https://www.youtube.com/embed/r7QDUPb2dCM" frameborder="0" allow="accelerometer; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</div><p>Tensor는 PyTorch에서 중요한 추상 데이터 자료형입니다. 이 interactive
notebook은 <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> 클래스에 대한 심층적인 소개를 제공합니다.</p>
<p>먼저 가장 중요한 것은 PyTorch 모듈을 import 하는 것입니다. 또한 몇 가지
예제에 사용할 math 모듈도 import 합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">math</span>
</pre></div>
</div>
<div class="section" id="tensor">
<h2>Tensor 생성하기<a class="headerlink" href="#tensor" title="이 제목에 대한 퍼머링크">¶</a></h2>
<p>tensor를 생성하는 가장 간단한 방법은 <code class="docutils literal notranslate"><span class="pre">torch.empty()</span></code> 를 호출하는 것입니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;torch.Tensor&#39;&gt;
tensor([[nan, nan, nan, nan],
        [nan, nan, nan, nan],
        [nan, nan, nan, nan]])
</pre></div>
</div>
<p>방금 무엇을 한 것인지 들여다봅시다:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch</span></code> 모듈에 있는 수많은 메소드 중 하나를 사용해서 tensor를 만들었습니다.</p></li>
<li><p>이 tensor는 3개의 행과 4개의 열을 가진 2차원 tensor입니다.</p></li>
<li><p>객체가 반환한 type은 <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> 이며 이는 <code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code> 의 별칭입니다.
기본적으로 PyTorch tensor는 32-bit 부동 소수점 표현 실수로 채워 집니다.
(아래에서 더 많은 데이터 자료형을 소개합니다)</p></li>
<li><p>생성한 tensor를 출력하면 아마 무작위 값을 볼 수 있을 것 입니다.
<code class="docutils literal notranslate"><span class="pre">torch.empty()</span></code> 는 tensor를 위한 메모리를 할당해 주지만 임의의 값으로 초기화하지는 않습니다
- 그렇기 때문에 할당 당시에 메모리가 가지고 있는 값을 보는 것입니다.</p></li>
</ul>
<p>간략하게 tensor와 tensor의 차원 수, 그리고 각 tensor의 용어에 대해 알아봅시다:</p>
<ul class="simple">
<li><p>때로는 1차원 tensor를 보게 될 것인데 이는 <em>vector</em> 라고 합니다.</p></li>
<li><p>이와 마찬가지로 2차원 tensor는 주로 <em>matrix</em> 라고 합니다.</p></li>
<li><p>2차원보다 큰 차원을 가진 것들은 일반적으로 그냥 tensor라고 합니다.</p></li>
</ul>
<p>코드를 작성하며 주로 tensor를 몇 가지 값으로 초기화하고 싶을 수가 있습니다.
일반적인 경우로는 모두 0으로 초기화하거나, 모두 1로 초기화하거나,
혹은 모두 무작위 값으로 초기화 할 때가 있는데,
<code class="docutils literal notranslate"><span class="pre">torch</span></code> 모듈은 이러한 모든 경우에 대한 메소드를 제공합니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">zeros</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zeros</span><span class="p">)</span>

<span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1729</span><span class="p">)</span>
<span class="n">random</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0., 0., 0.],
        [0., 0., 0.]])
tensor([[1., 1., 1.],
        [1., 1., 1.]])
tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]])
</pre></div>
</div>
<p>이 모든 팩토리 메소드들은 우리가 기대하던 것들을 모두 수행합니다 - 0으로 모두 채워 진 tensor,
1로 모두 채워 진 tensor 그리고 0과 1사이의 무작위 값으로 채워 진 tensor를 얻었습니다.</p>
<div class="section" id="tensor-seed">
<h3>무작위 Tensor와 Seed 사용하기<a class="headerlink" href="#tensor-seed" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>무작위 tensor에 대해 말하자면, 바로 앞에서 호출하는 <code class="docutils literal notranslate"><span class="pre">torch.manual_seed()</span></code> 를 눈치채셨나요?
특히 연구 환경에서 연구 결과의 재현 가능성에 대한 확신을 얻고 싶을 때,
모델의 학습 가중치와 같은 무작위 값을 가진 tensor로 초기화 하는 것은 흔하거나 종종 일어나는 일입니다.
직접 무작위 난수 생성기의 seed를 설정하는 것이 다음 방법입니다. 다음 코드를 보며 더 자세히 알아봅시다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1729</span><span class="p">)</span>
<span class="n">random1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random1</span><span class="p">)</span>

<span class="n">random2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random2</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1729</span><span class="p">)</span>
<span class="n">random3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random3</span><span class="p">)</span>

<span class="n">random4</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random4</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]])
tensor([[0.2332, 0.4047, 0.2162],
        [0.9927, 0.4128, 0.5938]])
tensor([[0.3126, 0.3791, 0.3087],
        [0.0736, 0.4216, 0.0691]])
tensor([[0.2332, 0.4047, 0.2162],
        [0.9927, 0.4128, 0.5938]])
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">random1</span></code> 과 <code class="docutils literal notranslate"><span class="pre">random3</span></code> 그리고 <code class="docutils literal notranslate"><span class="pre">random2</span></code> 과 <code class="docutils literal notranslate"><span class="pre">random4</span></code> ,
이 각각 서로 동일한 결과가 나온다는 것을 볼 수 있습니다.
무작위 난수 생성기의 seed를 수동으로 설정하면 난수가 재 설정되어 대부분의 환경에서
무작위 숫자에 의존하는 동일한 계산이 이루어지고 동일한 결과를 제공합니다.</p>
<p>보다 자세한 정보는 다음 문서를 참고하세요 <a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html">PyTorch documentation on
reproducibility</a>.</p>
</div>
<div class="section" id="tensor-shape">
<h3>Tensor의 shape<a class="headerlink" href="#tensor-shape" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>두 개 혹은 그 이상의 tensor에 대한 연산을 수행할 때, tensor들은 같은 shape를 필요로 합니다
- 다시 말해서 차원의 개수가 같아야 하고, 각 차원마다 원소의 수가 같아야 합니다.
그러기 위해서는 <code class="docutils literal notranslate"><span class="pre">torch.*_like()</span></code> 함수를 사용합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">empty_like_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">empty_like_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">empty_like_x</span><span class="p">)</span>

<span class="n">zeros_like_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zeros_like_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">zeros_like_x</span><span class="p">)</span>

<span class="n">ones_like_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ones_like_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ones_like_x</span><span class="p">)</span>

<span class="n">rand_like_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rand_like_x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rand_like_x</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([2, 2, 3])
tensor([[[nan, nan, nan],
         [nan, nan, nan]],

        [[nan, nan, nan],
         [nan, nan, nan]]])
torch.Size([2, 2, 3])
tensor([[[nan, nan, nan],
         [nan, nan, nan]],

        [[nan, nan, nan],
         [nan, nan, nan]]])
torch.Size([2, 2, 3])
tensor([[[0., 0., 0.],
         [0., 0., 0.]],

        [[0., 0., 0.],
         [0., 0., 0.]]])
torch.Size([2, 2, 3])
tensor([[[1., 1., 1.],
         [1., 1., 1.]],

        [[1., 1., 1.],
         [1., 1., 1.]]])
torch.Size([2, 2, 3])
tensor([[[0.6128, 0.1519, 0.0453],
         [0.5035, 0.9978, 0.3884]],

        [[0.6929, 0.1703, 0.1384],
         [0.4759, 0.7481, 0.0361]]])
</pre></div>
</div>
<p>위쪽의 코드 셀에 있는 것들 중에 첫 번째는 tensor에 있는 <code class="docutils literal notranslate"><span class="pre">.shape</span></code> 속성을 사용했습니다.
이 속성은 tensor의 각 차원 크기에 대한 리스트를 포함합니다
- 이 경우에, <code class="docutils literal notranslate"><span class="pre">x</span></code> 는 shape가 2 x 2 x 3 인 3차원 tensor입니다.</p>
<p>그 아래에는 <code class="docutils literal notranslate"><span class="pre">.empty_like()</span></code>, <code class="docutils literal notranslate"><span class="pre">.zeros_like()</span></code>,
<code class="docutils literal notranslate"><span class="pre">.ones_like()</span></code>, and <code class="docutils literal notranslate"><span class="pre">.rand_like()</span></code> 메소드를 호출 합니다.
<code class="docutils literal notranslate"><span class="pre">.shape</span></code> 속성을 통해서, 위의 메소드들이 동일한 차원값을 반환한다는 것을 검증할 수 있습니다.</p>
<p>여기서 다루는 tensor를 생성하는 마지막 방법은 PyTorch collection
형식의 데이터를 직접적으로 명시하는 것 입니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">some_constants</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">3.1415926</span><span class="p">,</span> <span class="mf">2.71828</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.61803</span><span class="p">,</span> <span class="mf">0.0072897</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">some_constants</span><span class="p">)</span>

<span class="n">some_integers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">17</span><span class="p">,</span> <span class="mi">19</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">some_integers</span><span class="p">)</span>

<span class="n">more_integers</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">more_integers</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[3.1416, 2.7183],
        [1.6180, 0.0073]])
tensor([ 2,  3,  5,  7, 11, 13, 17, 19])
tensor([[2, 4, 6],
        [3, 6, 9]])
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">torch.tensor()</span></code> 는 이미 Python tuple이나 list 형태로 이루어진 데이터를
가지고 있는 경우 tensor를 만들기 가장 쉬운 방법입니다.
위에서 본 것 처럼 중첩된 형태의 collection 자료형은 다차원 tensor가 결과로 나옵니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.tensor()</span></code> 는 데이터의 복사본을 생성합니다.</p>
</div>
</div>
<div class="section" id="id2">
<h3>Tensor 자료형<a class="headerlink" href="#id2" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>tensor의 자료형을 설정하는 것은 다양한 방식이 가능합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span> <span class="o">*</span> <span class="mf">20.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[1, 1, 1],
        [1, 1, 1]], dtype=torch.int16)
tensor([[ 0.9956,  1.4148,  5.8364],
        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)
tensor([[ 0,  1,  5],
        [11, 11, 11]], dtype=torch.int32)
</pre></div>
</div>
<p>tensor의 자료형을 설정하는 가장 단순한 방식은 생성할 때 선택적 인자를 사용하는 것 입니다.
위에 있는 cell의 첫 번째 줄을 보면, tensor <code class="docutils literal notranslate"><span class="pre">a</span></code> 를
<code class="docutils literal notranslate"><span class="pre">dtype=torch.int16</span></code> 자료형으로 설정했습니다. <code class="docutils literal notranslate"><span class="pre">a</span></code> 를 출력할 때,
<code class="docutils literal notranslate"><span class="pre">1.</span></code> 대신에 <code class="docutils literal notranslate"><span class="pre">1</span></code> 로 가득 찬 모습을 볼 수 있습니다
- 파이썬에서 아래 점이 없으면 실수 자료형이 아닌 정수 자료형을 의미합니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">a</span></code> 를 출력할 때 또 한가지 주목할 점은,
<code class="docutils literal notranslate"><span class="pre">dtype</span></code> 을 기본값 (32-bit 부동 소수점)
으로 남길 때와 다르게 tensor를 출력하는 경우
각 tensor의 <code class="docutils literal notranslate"><span class="pre">dtype</span></code> 을 명시한다는 것입니다.</p>
<p>tensor의 shape를 정수형 인자의 나열, 즉 이 인자를 tuple 자료형 형태로
묶는다는 것을 발견할 수 있습니다. 이것은 반드시 필요한 것은 아닙니다
- PyTorch에서는 첫 번째 인자로 tensor shape라는 값을 의미하는 라벨이 없는 정수 인자를 여러개를 받습니다 -
하지만 선택 인자를 추가했을 때, 이 방식은 코드를 더 읽기 쉽게 만들 수 있습니다.</p>
<p>자료형을 설정하는 다른 방법은 <code class="docutils literal notranslate"><span class="pre">.to()</span></code> 메소드랑 함께 사용하는 것 입니다.
위쪽 셀에서 평범한 방식으로 무작위 실수 자료형 tensor <code class="docutils literal notranslate"><span class="pre">b</span></code> 를 생성합니다.
이어서 <code class="docutils literal notranslate"><span class="pre">.to()</span></code> 메소드를 사용해서 <code class="docutils literal notranslate"><span class="pre">b</span></code> 를 32-bit 정수 자료형으로 변환한 <code class="docutils literal notranslate"><span class="pre">c</span></code> 를 생성합니다.
<code class="docutils literal notranslate"><span class="pre">c</span></code> 는 모든 <code class="docutils literal notranslate"><span class="pre">b</span></code> 의 값과 같은 값을 가지고 있지만 소수점 아래 자리를 버린다는 점이 다릅니다.</p>
<p>가능한 데이터 자료형은 다음을 포함합니다:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">torch.bool</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.int8</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.uint8</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.int16</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.int32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.int64</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.half</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.float</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.double</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.bfloat</span></code></p></li>
</ul>
</div>
</div>
<div class="section" id="id3">
<h2>PyTorch Tensor에서 산술 &amp; 논리 연산<a class="headerlink" href="#id3" title="이 제목에 대한 퍼머링크">¶</a></h2>
<p>지금까지 tensor를 생성하는 몇 가지 방식을 알아봤습니다…
이것을 가지고 무엇을 할 수 있을까요?</p>
<p>먼저 기본적인 산술 연산을 알아보고,
그 다음 tensor가 단순 스칼라 값과 어떻게 상호작용 하는지 알아봅시다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ones</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">twos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">threes</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">7</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="n">fours</span> <span class="o">=</span> <span class="n">twos</span> <span class="o">**</span> <span class="mi">2</span>
<span class="n">sqrt2s</span> <span class="o">=</span> <span class="n">twos</span> <span class="o">**</span> <span class="mf">0.5</span>

<span class="nb">print</span><span class="p">(</span><span class="n">ones</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">twos</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">threes</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fours</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sqrt2s</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[1., 1.],
        [1., 1.]])
tensor([[2., 2.],
        [2., 2.]])
tensor([[3., 3.],
        [3., 3.]])
tensor([[4., 4.],
        [4., 4.]])
tensor([[1.4142, 1.4142],
        [1.4142, 1.4142]])
</pre></div>
</div>
<p>위에서 볼 수 있듯이 tensor들과 스칼라 값 사이 산술연산,
예를 들면 덧셈, 뺄셈, 곱셈, 나눗셈 그리고 거듭제곱은
tensor의 각 원소에 나눠서 계산을 합니다.
이러한 연산의 결과는 tensor가 될 것이기 때문에,
<code class="docutils literal notranslate"><span class="pre">threes</span></code> 변수를 생성하는 줄에서 처럼
일반적인 연산자 우선순위 규칙과 함께 연산자를 연결할 수 있습니다.</p>
<p>두 tensor 사이 유사한 연산도 직관적으로 예상할 수 있는 방식으로 동작합니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">powers2</span> <span class="o">=</span> <span class="n">twos</span> <span class="o">**</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">powers2</span><span class="p">)</span>

<span class="n">fives</span> <span class="o">=</span> <span class="n">ones</span> <span class="o">+</span> <span class="n">fours</span>
<span class="nb">print</span><span class="p">(</span><span class="n">fives</span><span class="p">)</span>

<span class="n">dozens</span> <span class="o">=</span> <span class="n">threes</span> <span class="o">*</span> <span class="n">fours</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dozens</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 2.,  4.],
        [ 8., 16.]])
tensor([[5., 5.],
        [5., 5.]])
tensor([[12., 12.],
        [12., 12.]])
</pre></div>
</div>
<p>여기서 주목해야 할 점은 이전 코드 cell에 있는 모든 tensor는 동일한 shape를 가져야 한다는 것 입니다.
만약 서로 다른 shape를 가진 tensor끼리 이진 연산을 수행한다면 무슨 일이 일어날까요?</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>다음 cell은 run-time error가 발생합니다. 이것은 의도한 것입니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nv">a</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.rand<span class="o">(</span><span class="m">2</span>,<span class="w"> </span><span class="m">3</span><span class="o">)</span>
<span class="nv">b</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>torch.rand<span class="o">(</span><span class="m">3</span>,<span class="w"> </span><span class="m">2</span><span class="o">)</span>

print<span class="o">(</span>a<span class="w"> </span>*<span class="w"> </span>b<span class="o">)</span>
</pre></div>
</div>
</div>
<p>일반적인 경우에, 다른 shape의 tensor를 이러한 방식으로 연산할 수 없습니다.
심지어 위에 있는 cell에 있는 경우처럼 tensor가 서로 같은 개수의 원소를 가지고 있는 경우에도 연산할 수 없습니다.</p>
<div class="section" id="tensor-broadcasting">
<h3>개요: Tensor Broadcasting<a class="headerlink" href="#tensor-broadcasting" title="이 제목에 대한 퍼머링크">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>만약 NumPy의 ndarrays에서 사용하는 broadcasting 문법에 익숙하다면,
여기서도 같은 규칙이 적용된다는 것을 확인할 수 있습니다.</p>
</div>
<p>tensor는 같은 shape끼리만 연산이 가능하다는 규칙의 예외가 바로 <em>tensor broadcasting</em> 입니다.
다음은 그 예시입니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">doubled</span> <span class="o">=</span> <span class="n">rand</span> <span class="o">*</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">rand</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">doubled</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0.6146, 0.5999, 0.5013, 0.9397],
        [0.8656, 0.5207, 0.6865, 0.3614]])
tensor([[1.2291, 1.1998, 1.0026, 1.8793],
        [1.7312, 1.0413, 1.3730, 0.7228]])
</pre></div>
</div>
<p>여기서 무슨 트릭이 사용되고 있는 것일까요?
어떻게 2x4 tensor에 1x4 tensor를 곱한 값을 얻을 수 있을까요?</p>
<p>Broadcasting은 서로 비슷한 shape를 가진 tensor사이 연산을 수행하는 방법입니다.
위의 예시를 보면, 행의 값이 1이고, 열의 값이 4인 tensor가
행의 값이 2이고, 열의 값이 4인 tensor의 <em>모든 행</em> 에 곱하게 됩니다.</p>
<p>이것은 딥러닝에서 중요한 연산입니다.
일반적인 예시는 학습 가중치 tensor에 입력 tensor의 <em>배치</em> 를 곱하고,
배치의 각 인스턴스에 곱하기 연산을 개별적으로 적용한 이후
위의 (2, 4) * (1, 4) tensor연산의 결과가 (2, 4) shape tensor인 것처럼 -
동일한 shape의 학습 가중치 tensor를 반환하는 것입니다.</p>
<p>Broadcasting의 규칙은 다음과 같습니다:</p>
<ul class="simple">
<li><p>각 tensor는 최소한 1차원 이상을 반드시 가지고 있어야 합니다 - 빈 tensor는 사용할 수 없습니다.</p></li>
<li><p>두 tensor의 각 차원 크기 원소가 다음 조건을 만족하는지 확인하며 비교합니다. <em>이때 비교 순서는 맨 뒤에서부터 맨 앞으로 입니다;</em></p>
<ul>
<li><p>각 차원이 서로 동일합니다, <em>또는</em></p></li>
<li><p>각 차원중의 하나의 크기가 반드시 1입니다, <em>또는</em></p></li>
<li><p>tensor들 중 하나의 차원이 존재하지 않습니다.</p></li>
</ul>
</li>
</ul>
<p>이전에 봤던 것처럼,
물론 동일한 shape를 가진 Tensor들은 자명하게 “broadcastable” 합니다.</p>
<p>다음 예제는 위의 규칙을 준수하고
broadcasting을 허용하는 몇 가지 상황입니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span>     <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># 세번째와 두번째 차원이 a랑 동일하고, 첫번째 차원은 존재하지 않습니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># 세번째 차원 = 1이고, 두번째 차원은 a랑 동일합니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># 세번째 차원이 a랑 동일하고, 두번째 차원 = 1입니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]],

        [[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]],

        [[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]],

        [[0.6493, 0.2633],
         [0.4762, 0.0548],
         [0.2024, 0.5731]]])
tensor([[[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]],

        [[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]],

        [[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]],

        [[0.7191, 0.7191],
         [0.4067, 0.4067],
         [0.7301, 0.7301]]])
tensor([[[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]],

        [[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]],

        [[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]],

        [[0.6276, 0.7357],
         [0.6276, 0.7357],
         [0.6276, 0.7357]]])
</pre></div>
</div>
<p>위의 예시에 있는 각 tensor의 값을 자세히 살펴봅시다:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">b</span></code> 를 만드는 곱셈 연산은
<code class="docutils literal notranslate"><span class="pre">a</span></code> 의 모든 “계층” 에 broadcast 되었습니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">c</span></code> 에 대해서, 연산은 <code class="docutils literal notranslate"><span class="pre">a</span></code> 의 모든 계층과 행에 대해서 broadcast 되었습니다
- 모든 열은 3개의 원소값 모두 동일합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">d</span></code> 에 대해서, 연산이 이전과 반대로 모든 계층과 열에 대해서 수행합니다
- 이재 모든 <em>행</em> 이 동일합니다.</p></li>
</ul>
<p>broadcasting에 대한 더 많은 정보는, <a class="reference external" href="https://pytorch.org/docs/stable/notes/broadcasting.html">PyTorch
documentation</a>
에 있는 주제를 참고하세요.</p>
<p>다음 예시는 broadcasting 시도가 실패한 사례입니다:</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>다음 cell에서는 run-time error가 발생합니다. 이것은 의도한 것입니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span>     <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>    <span class="c1"># 차원은 반드시 맨 뒤 원소부터 맨 앞 원소로 차례대로 맞춰야 합니다.</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 세번째와 두번째 차원 모두 서로 다릅니다.</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="p">))</span>   <span class="c1"># 비어있는 tensor는 broadcast 할 수 없습니다.</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="id4">
<h3>Tensor를 사용하는 다양한 연산들<a class="headerlink" href="#id4" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>PyTorch tensor는 tensor들 끼리 수행할 수 있는 300개 이상의
연산을 가지고 있습니다.</p>
<p>다음 작은 예시는 주로 사용하는 연산 종류 몇 개를 보여줍니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># 공용 함수</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Common functions:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">))</span>

<span class="c1"># 삼각 함수와 그 역함수들</span>
<span class="n">angles</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">sines</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>
<span class="n">inverses</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">asin</span><span class="p">(</span><span class="n">sines</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Sine and arcsine:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sines</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">inverses</span><span class="p">)</span>

<span class="c1"># 비트 연산</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Bitwise XOR:&#39;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">11</span><span class="p">])</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bitwise_xor</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">))</span>

<span class="c1"># 비교 연산:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Broadcasted, element-wise equality comparison:&#39;</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]])</span>
<span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># 많은 비교 연산자들은 broadcasting을 지원합니다!</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">))</span> <span class="c1"># bool 자료형을 가진 tensor를 반환합니다.</span>

<span class="c1"># 차원 감소 연산:</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Reduction ops:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>        <span class="c1"># 단일 원소 tensor를 반환합니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">d</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> <span class="c1"># 반환한 tensor로부터 값을 추출합니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>       <span class="c1"># 평균</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>        <span class="c1"># 표준 편차</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">d</span><span class="p">))</span>       <span class="c1"># 모든 숫자의 곱</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])))</span> <span class="c1"># 중복되지 않은 값들을 걸러냅니다.</span>

<span class="c1"># 벡터와 선형 대수 연산</span>
<span class="n">v1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>         <span class="c1"># x축 단위 벡터</span>
<span class="n">v2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>         <span class="c1"># y축 단위 벡터</span>
<span class="n">m1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>                   <span class="c1"># 무작위 행렬</span>
<span class="n">m2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]])</span> <span class="c1"># 단위 행렬에 3을 곱한 결과</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Vectors &amp; Matrices:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cross</span><span class="p">(</span><span class="n">v2</span><span class="p">,</span> <span class="n">v1</span><span class="p">))</span> <span class="c1"># z축 단위 벡터의 음수값 (v1 x v2 == -v2 x v1)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m1</span><span class="p">)</span>
<span class="n">m3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m3</span><span class="p">)</span>                  <span class="c1"># m1 행렬을 3번 곱한 결과</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">m3</span><span class="p">))</span>       <span class="c1"># 특이값 분해</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Common functions:
tensor([[0.9238, 0.5724, 0.0791, 0.2629],
        [0.1986, 0.4439, 0.6434, 0.4776]])
tensor([[-0., -0., 1., -0.],
        [-0., 1., 1., -0.]])
tensor([[-1., -1.,  0., -1.],
        [-1.,  0.,  0., -1.]])
tensor([[-0.5000, -0.5000,  0.0791, -0.2629],
        [-0.1986,  0.4439,  0.5000, -0.4776]])

Sine and arcsine:
tensor([0.0000, 0.7854, 1.5708, 2.3562])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
tensor([0.0000, 0.7854, 1.5708, 0.7854])

Bitwise XOR:
tensor([3, 2, 1])

Broadcasted, element-wise equality comparison:
tensor([[ True, False],
        [False, False]])

Reduction ops:
tensor(4.)
4.0
tensor(2.5000)
tensor(1.2910)
tensor(24.)
tensor([1, 2])

Vectors &amp; Matrices:
tensor([ 0.,  0., -1.])
tensor([[0.7375, 0.8328],
        [0.8444, 0.2941]])
tensor([[2.2125, 2.4985],
        [2.5332, 0.8822]])
torch.return_types.svd(
U=tensor([[-0.7889, -0.6145],
        [-0.6145,  0.7889]]),
S=tensor([4.1498, 1.0548]),
V=tensor([[-0.7957,  0.6056],
        [-0.6056, -0.7957]]))
</pre></div>
</div>
<p>이것은 수많은 연산의 일부분입니다.
더 자세한 내용이나 수학 함수의 전체적인 목록은, 다음
<a class="reference external" href="https://pytorch.org/docs/stable/torch.html#math-operations">documentation</a>
를 읽어주세요.</p>
</div>
<div class="section" id="id5">
<h3>Tensor의 값을 변경하기<a class="headerlink" href="#id5" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>대부분 tensor들의 이진 연산은 제3자의 새로운 tensor를 생성합니다.
<code class="docutils literal notranslate"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">a</span> <span class="pre">*</span> <span class="pre">b</span></code> ( <code class="docutils literal notranslate"><span class="pre">a</span></code> 와 <code class="docutils literal notranslate"><span class="pre">b</span></code> 는 tensor)연산을 수행할 때,
새로운 tensor <code class="docutils literal notranslate"><span class="pre">c</span></code> 는 다른 tensor와 구별되는 메모리 영역을 차지하게 됩니다.</p>
<p>그럼에도 불구하고 tensor의 값을 변경하고 싶은 순간이 있을 수 있습니다 -
예를 들어, 중간 연산 결과 값을 버릴 수 있는 각 원소 단위 연산을 수행하는 경우가 있습니다.
이런 연산을 위해, 대부분의 수학 함수들은 tensor 내부의 값을
변경할 수 있는 함수 이름 맨 뒤에 밑줄 (<code class="docutils literal notranslate"><span class="pre">_</span></code>)이 추가된 버전을 가지고 있습니다.</p>
<p>예시:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;a:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">a</span><span class="p">))</span>   <span class="c1"># 이 연산은 메모리에 새로운 tensor를 생성합니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>              <span class="c1"># a는 변하지 않습니다.</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">,</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">b:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sin_</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>  <span class="c1"># 밑줄에 주목하세요.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>              <span class="c1"># b가 변합니다.</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>a:
tensor([0.0000, 0.7854, 1.5708, 2.3562])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
tensor([0.0000, 0.7854, 1.5708, 2.3562])

b:
tensor([0.0000, 0.7854, 1.5708, 2.3562])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
tensor([0.0000, 0.7071, 1.0000, 0.7071])
</pre></div>
</div>
<p>산술 연산에서, 비슷한 행동을 하는 함수가 있습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Before:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">After adding:&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">After multiplying&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">b</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Before:
tensor([[1., 1.],
        [1., 1.]])
tensor([[0.3788, 0.4567],
        [0.0649, 0.6677]])

After adding:
tensor([[1.3788, 1.4567],
        [1.0649, 1.6677]])
tensor([[1.3788, 1.4567],
        [1.0649, 1.6677]])
tensor([[0.3788, 0.4567],
        [0.0649, 0.6677]])

After multiplying
tensor([[0.1435, 0.2086],
        [0.0042, 0.4459]])
tensor([[0.1435, 0.2086],
        [0.0042, 0.4459]])
</pre></div>
</div>
<p>이러한 내부의 값을 변경하는 산술 함수는 다른 많은 함수들
(e.g., <code class="docutils literal notranslate"><span class="pre">torch.sin()</span></code>)처럼 <code class="docutils literal notranslate"><span class="pre">torch</span></code> 모듈의 메소드가 아니라
<code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> 객체의 메소드인 점에 주목해야 합니다.
<code class="docutils literal notranslate"><span class="pre">a.add_(b)</span></code> 와 같은 경우처럼, <em>메소드를 호출하는 tensor는 값이 변경됩니다.</em></p>
<p>이미 존재하고 있는 메모리에 할당된 tensor에 계산 결과값을 저장하는 또 다른 옵션이 있습니다.
tensor를 생성하는 메소드 뿐만 아니라 지금까지 이 문서에서 봤던 수많은 함수나 메소드는
결과 값을 받는 특정 tensor를 명시하는 <code class="docutils literal notranslate"><span class="pre">out</span></code> 이라는 인자를 가지고 있습니다.
만약 <code class="docutils literal notranslate"><span class="pre">out</span></code> tensor가 알맞은 shape와 <code class="docutils literal notranslate"><span class="pre">dtype</span></code> 을 가지고 있다면,
새로운 메모리 할당 없이 결과값이 저장됩니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">old_id</span> <span class="o">=</span> <span class="nb">id</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>                <span class="c1"># c의 값이 변경되었습니다.</span>

<span class="k">assert</span> <span class="n">c</span> <span class="ow">is</span> <span class="n">d</span>           <span class="c1"># c와 d가 서로 단순히 같은 값을 가지는지가 아니라 같은 객체인지 테스트합니다.</span>
<span class="k">assert</span> <span class="nb">id</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="n">old_id</span>  <span class="c1"># 새로운 c는 이전 객체와 확실히 같은 객체입니다.</span>

<span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">c</span><span class="p">)</span> <span class="c1"># 다시 한번 생성해봅시다!</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>                <span class="c1"># c의 값이 다시 바뀌었습니다.</span>
<span class="k">assert</span> <span class="nb">id</span><span class="p">(</span><span class="n">c</span><span class="p">)</span> <span class="o">==</span> <span class="n">old_id</span>  <span class="c1"># 하지만 여전히 같은 객체네요!</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0., 0.],
        [0., 0.]])
tensor([[0.3653, 0.8699],
        [0.2364, 0.3604]])
tensor([[0.0776, 0.4004],
        [0.9877, 0.0352]])
</pre></div>
</div>
</div>
</div>
<div class="section" id="id6">
<h2>Tensor를 복사하기<a class="headerlink" href="#id6" title="이 제목에 대한 퍼머링크">¶</a></h2>
<p>파이썬의 다른 객체와 마찬가지로 변수에 tensor를 할당하는 것은
변수가 tensor의 <em>label</em> 이 되고 값을 복사하지 않습니다. 다음 예시를 보시죠:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span>

<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">561</span>  <span class="c1"># a의 값을 바꾸면...</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>       <span class="c1"># ...b의 값이 바뀝니다.</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[  1., 561.],
        [  1.,   1.]])
</pre></div>
</div>
<p>하지만 만약 우리가 작업할 별도의 데이터 복사본을 원하면 어떻게 해야할까요?
<code class="docutils literal notranslate"><span class="pre">clone()</span></code> 메소드가 당신이 찾던 해답이 될 것입니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

<span class="k">assert</span> <span class="n">b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">a</span>      <span class="c1"># 메모리 상의 다른 객체입니다...</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>  <span class="c1"># ...하지만 여전히 같은 값을 가지고 있네요!</span>

<span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">561</span>          <span class="c1"># a가 변경되었습니다...</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>               <span class="c1"># ...하지만 여전히 b는 이전 값을 가지고 있네요.</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[True, True],
        [True, True]])
tensor([[1., 1.],
        [1., 1.]])
</pre></div>
</div>
<p><strong>``clone()`` 메소드를 사용할 때 알아야 할 중요한 점이 있습니다.</strong>
만약 source tensor가 autograd를 가진다면 clone이 가능합니다.
<strong>이 부분은 autograd와 관련된 동영상에서 더 깊이 다룰 것 입니다.</strong>
하지만 만약 자세한 내용을 간단히 알고 싶다면 계속 설명하겠습니다.</p>
<p><em>대부분의 경우에서 이것이 바로 여러분이 원하는 것입니다.</em>
예를 들어, 만약 여러분의 모델이 그 모델의 <code class="docutils literal notranslate"><span class="pre">forward()</span></code> 메소드에 여러 갈래의 계산 경로가 있고
원본 tensor와 그것의 복제본 <em>모두</em> 가 모델의 출력에 기여를 한다면,
두 tensor에 대한 autograd를 설정하는 모델 학습을 활성화 합니다.
만약 여러분의 source tensor가 autograd를 사용할 수 있다면
(일반적으로 학습 가중치의 집합이거나, 가중치를 포함하는 계산에서 파생된 경우),
여러분이 원하는 결과를 얻을 수 있습니다.</p>
<p>반면에 원본 tensor나 그것의 복제본 <em>모두</em> 가 변화도를 추적할 필요가 없다면,
source tensor의 autograd가 꺼져있다면
clone을 사용할 수 있습니다.</p>
<p>그러나 <em>세번째 경우</em> 가 있습니다:
기본적으로 변화도가 모든 것을 위해 켜져있지만 일부 지표를 생성하기 위해서
스트림 중간에서 일부 값을 생성하고 싶어 하는
여러분 모델의 <code class="docutils literal notranslate"><span class="pre">forward()</span></code> 함수에서 계산을 수행한다고 상상해 보세요.
이 경우에는 변화도를 추적하기 위해서 source tensor의 복제본을 원하지 <em>않을</em> 수 있습니다
- 성능이 autograd의 히스토리 추적 기능을 끄면서 향상됩니다.
이 경우를 위해서는 source tensor에 <code class="docutils literal notranslate"><span class="pre">.detach()</span></code> 메소드를 사용할 수 있습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># autograd를 켭니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]], requires_grad=True)
tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]], grad_fn=&lt;CloneBackward0&gt;)
tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]])
tensor([[0.0905, 0.4485],
        [0.8740, 0.2526]], requires_grad=True)
</pre></div>
</div>
<p>여기서 무슨 일이 일어나는걸까요?</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 를  <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 옵션을 킨 상태로 생성합니다.
<strong>아직 이 선택적 인자를 다루지 않았지만, autograd 단원 동안만 다룰 것입니다.</strong></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 를 출력할 때, <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 속성을 가지고 있다고 알려줍니다 -
이 뜻은 autograd와 계산 히스토리 추적 기능을 켠다는 것입니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 를 복제하고 그것을 <code class="docutils literal notranslate"><span class="pre">b</span></code> 라고 라벨을 붙입니다. <code class="docutils literal notranslate"><span class="pre">b</span></code> 를 출력할 때,
그것의 계산 히스토리가 추적되고 있다는 것을 확인할 수 있습니다 -
<code class="docutils literal notranslate"><span class="pre">a</span></code> 의 autograd 설정에 내장되어 있는 기능이며, 이 설정은 계산 히스토리에 추가합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">a</span></code> 를 <code class="docutils literal notranslate"><span class="pre">c</span></code> 에 복제를 하지만 <code class="docutils literal notranslate"><span class="pre">detach()</span></code> 를 먼저 호출을 합니다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">c</span></code> 를 출력합니다. 계산 히스토리가 없다는 것을 확인할 수 있고,
<code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code> 옵션이 없다는 것 또한 확인할 수 있습니다.</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">detach()</span></code> 메소드는 <em>tensor의 계산 히스토리로 부터 tensor를 떼어냅니다.</em>
이 메소드의 의미는 “메소드 뒤에 어떤 것이든 와도 autograd를 끈 것처럼 작동하라.“ 라는 뜻입니다.
<code class="docutils literal notranslate"><span class="pre">a</span></code> 를 변경하지 <em>않고</em> 이 메소드를 수행합니다 -
마지막에 <code class="docutils literal notranslate"><span class="pre">a</span></code> 를 다시 출력할 때, 여전히 <code class="docutils literal notranslate"><span class="pre">a</span></code> 가 가진 <code class="docutils literal notranslate"><span class="pre">requires_grad=True</span></code>
속성이 남아 있다는 것을 확인할 수 있습니다.</p>
</div>
<div class="section" id="gpu">
<h2>GPU 환경으로 이동하기<a class="headerlink" href="#gpu" title="이 제목에 대한 퍼머링크">¶</a></h2>
<p>PyTorch의 주된 장점중 하나는 CUDA가 호환되는 Nvidia GPU에서의 강력한 성능 가속화입니다.
(“CUDA” 는 <em>Compute Unified Device Architecture</em> 의 약자이며,
병렬 컴퓨팅을 위한 Nvidia의 플랫폼입니다.)
지금까지 모든 작업을 CPU에서 처리했습니다. 어떻게 더 빠른 하드웨어로 이동할 수 있을까요?</p>
<p>먼저 <code class="docutils literal notranslate"><span class="pre">is_available()</span></code> 메소드를 사용해서 GPU가 사용 가능한지 아닌지 확인해야 합니다.</p>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>만약 CUDA가 호환되는 GPU가 없고 CUDA 드라이버가 설치되어있지 않다면
이 섹션에서의 실행 가능한 cell은 어떤 GPU와 관련된 코드도 실행할 수 없습니다.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;We have a GPU!&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sorry, CPU only.&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>We have a GPU!
</pre></div>
</div>
<p>일단 1개 혹은 그 이상의 GPU가 사용 가능하다는 것을 확인했다면,
데이터를 GPU가 확인할 수 있는 어떤 공간에 저장할 필요가 있습니다.
CPU는 컴퓨터의 RAM에서 데이터를 이용해서 계산을 수행합니다.
GPU는 전용 메모리가 연결되어 있습니다. 해당 장치에서 계산을 수행하고 싶을 때마다
계산에 필요한 <em>모든</em> 데이터를 GPU장치가 접근 가능한 메모리로 이동해야 합니다.
(평소에는 “GPU가 접근 가능한 메모리로 데이터를 이동한다“
를 “데이터를 GPU로 옮긴다“ 라고 줄여서 말합니다.)</p>
<p>목적 장치에서 데이터를 가져오는 다양한 방법이 있습니다.
객체를 생성할 때 데이터를 가져올 수 있습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">gpu_rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">gpu_rand</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Sorry, CPU only.&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0.3344, 0.2640],
        [0.2119, 0.0582]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
<p>기본적으로 새로운 tensor는 CPU에 생성됩니다. 따라서 tensor를 GPU에 생성하고 싶을 때
<code class="docutils literal notranslate"><span class="pre">device</span></code> 선택 인자를 반드시 명시해줘야 합니다.
새로운 tensor를 출력할 때, (만약 CPU에 존재하지 않는다면)
PyTorch는 어느 장치에 객체가 있는지 알려준다는 것을 확인할 수 있습니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.cuda.device_count()</span></code> 를 사용해서 GPU의 개수를 조회할 수 있습니다.
만약 1개보다 많은 GPU를 가지고 있다면, 각 GPU를 인덱스로 지정할 수 있습니다:
<code class="docutils literal notranslate"><span class="pre">device='cuda:0'</span></code>, <code class="docutils literal notranslate"><span class="pre">device='cuda:1'</span></code>, 와 같이 말이죠.</p>
<p>코딩을 할 때, 어디에서나 장치 이름을 문자열 상수로 지정하는 것은 상당히 유지 보수에 취약합니다.
CPU 하드웨어나 GPU 하드웨어 어떤 것을 사용하는지에 관계없이 여러분의 코드는 잘 작동해야 합니다.
문자열 대신에 tensor를 저장할 장치 핸들러를 생성하는 것으로 유지 보수가 쉬운 코드를 작성할 수 있습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">my_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">my_device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Device: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">my_device</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">my_device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Device: cuda
tensor([[0.0024, 0.6778],
        [0.2441, 0.6812]], device=&#39;cuda:0&#39;)
</pre></div>
</div>
<p>만약 한 장치에 tensor가 있을 때, <code class="docutils literal notranslate"><span class="pre">to()</span></code> 메소드를 사용해서 다른 장치로 이동할 수 있습니다.
다음 코드는 CPU에 tensor를 생성하고, 이전 cell에서 얻은 장치 핸들러로 tensor를 이동합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">my_device</span><span class="p">)</span>
</pre></div>
</div>
<p>2개 혹은 그 이상의 tensor를 포함한 계산을 하기 위해서는
<em>모든 tensor가 같은 장치에 있어야 한다</em> 는 것을 아는 것이 중요합니다.
다음 코드는 GPU 장치의 사용 가능 여부와 관계없이 runtime error를 발생할 것입니다:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;gpu&#39;</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span>  <span class="c1"># 오류가 발생할 것입니다.</span>
</pre></div>
</div>
</div>
<div class="section" id="id7">
<h2>Tensor의 shape 다루기<a class="headerlink" href="#id7" title="이 제목에 대한 퍼머링크">¶</a></h2>
<p>때로는 tensor의 shape를 변환할 필요가 있습니다.
아래에 있는 몇 가지 흔한 경우와 함께 tensor의 shape를 다루는 방법에 대해 알아볼 것 입니다.</p>
<div class="section" id="id8">
<h3>차원의 개수 변경하기<a class="headerlink" href="#id8" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>차원의 개수를 변경할 필요가 있는 한가지 경우는 모델의 입력에 단일 인스턴스를 전달할 때 입니다.
PyTorch 모델은 일반적으로 입력에 <em>배치</em> 가 들어오기를 기대합니다.</p>
<p>예를 들어, 3개의 색깔 채널을 가진 226픽셀 정사각형 이미지인 3 x 226 x 226 개 데이터와
함께 작동하는 모델을 가지고 있다고 상상해보세요.
이미지를 불러오고 tensor로 변환하면 <code class="docutils literal notranslate"><span class="pre">(3,</span> <span class="pre">226,</span> <span class="pre">226)</span></code> shape를 가진 tensor가 됩니다.
그럼에도 불구하고 이 모델은 <code class="docutils literal notranslate"><span class="pre">(N,</span> <span class="pre">3,</span> <span class="pre">226,</span> <span class="pre">226)</span></code> shape를 가진 tensor를 입력으로 기대합니다.
이때 <code class="docutils literal notranslate"><span class="pre">N</span></code> 은 배치에 포함된 이미지의 개수입니다. 그렇다면 어떻게 한 배치를 만들 수 있을까요?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">226</span><span class="p">,</span> <span class="mi">226</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 226, 226])
torch.Size([1, 3, 226, 226])
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> 메소드는 크기가 1인 차원을 추가합니다.
<code class="docutils literal notranslate"><span class="pre">unsqueeze(0)</span></code> 는 새로운 0번째 차원을 추가합니다
- 이제 한 배치를 가지게 되었습니다!</p>
<p>이게 <em>un</em>squeezing이면, squeezing은 무슨 뜻 일까요?
여기서는 차원을 하나 확장해도 tensor에 있는 원소의 개수는 변하지
<em>않는다</em> 는 사실을 이용하고 있습니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[[[[0.2347]]]]])
</pre></div>
</div>
<p>위의 예제에 이어서 각 입력 값에 대한 모델의 출력 값이 20개의 원소를 가진 vector라고 생각해봅시다.
그렇다면 <code class="docutils literal notranslate"><span class="pre">N</span></code> 이 입력 배치에 있는 인스턴스의 개수라고 할 때,
출력 값의 shape는 <code class="docutils literal notranslate"><span class="pre">(N,</span> <span class="pre">20)</span></code> 라고 기대할 수 있습니다.
이 뜻은 입력으로 단일 배치가 들어왔을 때,
<code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">20)</span></code> 의 shape를 가진 출력 값을 얻는다는 것 입니다.</p>
<p>만약 그저 20개의 원소를 가진 벡터와 같이
- <em>배치 shape가 아닌</em> 연산 결과를 얻고 싶으면 어떻게 해야할까요?</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">d</span> <span class="o">=</span> <span class="n">c</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([1, 20])
tensor([[0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367,
         0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769,
         0.2792, 0.3277]])
torch.Size([20])
tensor([0.1899, 0.4067, 0.1519, 0.1506, 0.9585, 0.7756, 0.8973, 0.4929, 0.2367,
        0.8194, 0.4509, 0.2690, 0.8381, 0.8207, 0.6818, 0.5057, 0.9335, 0.9769,
        0.2792, 0.3277])
torch.Size([2, 2])
torch.Size([2, 2])
</pre></div>
</div>
<p>결과로 나온 shape로 부터 2차원 tensor가 이제 1차원으로 바뀐 것을 볼 수 있고,
위에 있는 cell의 결과를 자세히 보면
추가적인 차원을 가졌기 때문에
<code class="docutils literal notranslate"><span class="pre">a</span></code> 를 출력하는 것에서 “추가” 대괄호 집합 <code class="docutils literal notranslate"><span class="pre">[]</span></code> 을 볼 수 있습니다.</p>
<p>오직 차원의 값이 1인 경우에만 <code class="docutils literal notranslate"><span class="pre">squeeze()</span></code> 를 사용할 수 있습니다.
<code class="docutils literal notranslate"><span class="pre">c</span></code> 에서 크기가 2인 차원을 squeeze 하려고 하는 위 예시를 보면,
처음 그 shape로 다시 돌아온다는 사실을 알 수 있습니다.
<code class="docutils literal notranslate"><span class="pre">squeeze()</span></code> 와 <code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> 를 호출하는 것은 오직 차원의 크기가 1일 때만 작동합니다.
왜냐하면 이 경우가 아니면 tensor의 원소 개수가 바뀌기 때문입니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> 는 broadcasting을 쉽게 하는 경우에도 사용합니다.
다음 코드를 보고 이전 예시를 떠올려 보세요:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span>     <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># 3번째 차원은 1이고, 2번째 차원은 a와 동일합니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
<p>broadcast의 순수한 효과는 차원 0과 차원 2에 대한 연산을 broadcast해서
무작위 3 x 1 shape의 tensor를 <code class="docutils literal notranslate"><span class="pre">a</span></code> 의 원소 개수가 3인 모든 열에 곱하는 것이었습니다.</p>
<p>만약 무작위 벡터가 오직 3개의 원소만을 가지면 어떻게 될까요?
broadcast를 할 능력을 잃어버리게 됩니다, 왜냐하면 마지막 차원이
broadcasting 규칙에 맞지 않기 때문입니다.
하지만 <code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> 가 도와줍니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span>   <span class="mi">3</span><span class="p">)</span>     <span class="c1"># a * b를 시도하는 것은 runtime error가 발생합니다.</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">b</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>       <span class="c1"># 끝에 새로운 차원을 추가해서 2차원 tensor로 바꿉니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">c</span><span class="p">)</span>             <span class="c1"># broadcasting이 다시 작동합니다!</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 1])
tensor([[[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]],

        [[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]],

        [[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]],

        [[0.1891, 0.1891],
         [0.3952, 0.3952],
         [0.9176, 0.9176]]])
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">squeeze()</span></code> 와 <code class="docutils literal notranslate"><span class="pre">unsqueeze()</span></code> 메소드는 tensor 자체의 값을 변경하는
<code class="docutils literal notranslate"><span class="pre">squeeze_()</span></code> 와 <code class="docutils literal notranslate"><span class="pre">unsqueeze_()</span></code> 또한 가지고 있습니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">batch_me</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">226</span><span class="p">,</span> <span class="mi">226</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch_me</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">batch_me</span><span class="o">.</span><span class="n">unsqueeze_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch_me</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([3, 226, 226])
torch.Size([1, 3, 226, 226])
</pre></div>
</div>
<p>때로는 원소의 개수와 원소의 값을 여전히 유지하면서
tensor의 shape를 한번에 바꾸고 싶을 때가 있습니다.
모델의 합성곱 계층과 선형 계층 사이 인터페이스에서 이러한 상황이 발생합니다
- 이 상황은 이미지 분류 모델에서 흔히 일어나는 일입니다.
합성곱 커널은 <em>특성의 수 x 너비 x 높이</em> shpae의 tensor를 출력 값으로 생성하지만
이후에 있는 선형 계층은 입력 값으로 1차원을 기대합니다.
여러분이 요청한 차원에 입력 tensor가 가진 원소와 같은 개수를 생성하는
<code class="docutils literal notranslate"><span class="pre">reshape()</span></code> 를 여러분을 위해서 제공합니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">output3d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output3d</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">input1d</span> <span class="o">=</span> <span class="n">output3d</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">input1d</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># torch 모듈에 있는 메소드에 대해서도 호출할 수 있습니다.</span>
<span class="nb">print</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">output3d</span><span class="p">,</span> <span class="p">(</span><span class="mi">6</span> <span class="o">*</span> <span class="mi">20</span> <span class="o">*</span> <span class="mi">20</span><span class="p">,))</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([6, 20, 20])
torch.Size([2400])
torch.Size([2400])
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">참고</p>
<p>위에 있는 cell의 마지막 줄에 있는 인자 <code class="docutils literal notranslate"><span class="pre">(6</span> <span class="pre">*</span> <span class="pre">20</span> <span class="pre">*</span> <span class="pre">20,)</span></code> 는
PyTorch는 tensor shape를 나타낼 때 <strong>tuple</strong> 을 기대하기 때문입니다.
하지만 shape가 메소드의 첫번째 인자라면 - 연속된 정수라고 속여서 사용할 수 있습니다.
여기에서는 메소드에게 이 인자가 진짜 1개 원소를 가진 튜플이라고 알려주기 위해서
편의상 소괄호와 콤마를 추가해야 합니다.</p>
</div>
<p><code class="docutils literal notranslate"><span class="pre">reshape()</span></code> 는 tensor를 바라보는 <em>관점</em> 을 변경합니다.
- 즉, 메모리의 같은 지역을 바라보는 서로 다른 관점을 가진 tensor 객체라는 뜻입니다.
<em>이 내용은 정말 중요합니다:</em> source tensor에 어떠한 변화가 있으면
<code class="docutils literal notranslate"><span class="pre">clone()</span></code> 을 사용하지 않는 한, 해당 tensor를 바라보고 있는 다른 객체 또한
값이 변한다는 뜻 입니다.</p>
<p>해당 소개의 범위를 벗어난 조건 <em>들</em> 이 있습니다.
그것은 <code class="docutils literal notranslate"><span class="pre">reshape()</span></code> 가 data의 복사본을 가진 tensor를 반환 해야 한다는 것 입니다.
더 많은 정보는 다음 문서를 참고하세요
<a class="reference external" href="https://pytorch.org/docs/stable/torch.html#torch.reshape">docs</a>.</p>
</div>
</div>
<div class="section" id="numpy">
<h2>NumPy로 변환<a class="headerlink" href="#numpy" title="이 제목에 대한 퍼머링크">¶</a></h2>
<p>위에 있는 broadcasting 부분에서, PyTorch의 broadcast
문법은 Numpy와 호환 가능하다고 말했었습니다
- 하지만 PyTorch와 NumPy 사이 유사성은 우리가 생각한 것 보다 더욱 깊습니다.</p>
<p>만약 NumPy의 ndarrays에 저장되어 있는 데이터를 사용하는
머신 러닝 혹은 과학 분야와 관련된 코드를 가지고 있다면,
같은 데이터를 PyTorch의 GPU 가속을 사용할 수 있고
머신 러닝 모델을 만드는데 필요한 효과적인 추상화를 제공하는
PyTorch tensor로 표현하고 싶을 수 있습니다.
ndarray와 PyTorch tensor끼리 바꾸는 것은 쉽습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">numpy_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span>

<span class="n">pytorch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">numpy_array</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytorch_tensor</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[1. 1. 1.]
 [1. 1. 1.]]
tensor([[1., 1., 1.],
        [1., 1., 1.]], dtype=torch.float64)
</pre></div>
</div>
<p>PyTorch는 NumPy array와 같은 shape의 tensor를 생성하고, 같은 데이터를 포함합니다.
심지어 NumPy의 기본적인 64비트 실수 데이터 자료형을 유지합니다.</p>
<p>PyTorch에서 NumPy로 변환은 다른 방식을 사용해서 쉽게 할 수 있습니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pytorch_rand</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytorch_rand</span><span class="p">)</span>

<span class="n">numpy_rand</span> <span class="o">=</span> <span class="n">pytorch_rand</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy_rand</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[0.8716, 0.2459, 0.3499],
        [0.2853, 0.9091, 0.5695]])
[[0.87163675 0.2458961  0.34993553]
 [0.2853077  0.90905803 0.5695162 ]]
</pre></div>
</div>
<p>이러한 변환된 객체들은 해당 객체의 source 객체가 위치한
<em>메모리의 같은 공간</em> 을 사용한다는 점을 아는 것이 중요합니다.
이것은 한 객체가 변하면 다른 것에 영향을 준다는 의미입니다:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">numpy_array</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">23</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pytorch_tensor</span><span class="p">)</span>

<span class="n">pytorch_rand</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">17</span>
<span class="nb">print</span><span class="p">(</span><span class="n">numpy_rand</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>tensor([[ 1.,  1.,  1.],
        [ 1., 23.,  1.]], dtype=torch.float64)
[[ 0.87163675  0.2458961   0.34993553]
 [ 0.2853077  17.          0.5695162 ]]
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.178 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-beginner-introyt-tensors-deeper-tutorial-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/be017e7b39198fdf668c138fd8d57abe/tensors_deeper_tutorial.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tensors_deeper_tutorial.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/63a0f0fc7b3ffb15d3a5ac8db3d521ee/tensors_deeper_tutorial.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tensors_deeper_tutorial.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="autogradyt_tutorial.html" class="btn btn-neutral float-right" title="The Fundamentals of Autograd" accesskey="n" rel="next">Next <img src="../../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="introyt1_tutorial.html" class="btn btn-neutral" title="PyTorch 소개" accesskey="p" rel="prev"><img src="../../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  
    <hr class="community-hr hr-top" />
    <div class="community-container">
      <div class="community-prompt">더 궁금하시거나 개선할 내용이 있으신가요? 커뮤니티에 참여해보세요!</div>
      <div class="community-link"><a href="https://discuss.pytorch.kr/" aria-label="PyTorchKoreaCommunity">한국어 커뮤니티 바로가기</a></div>
    </div>
    <hr class="community-hr hr-bottom"/>

    <hr class="rating-hr hr-top" />
    <div class="rating-container">
      <div class="rating-prompt">이 튜토리얼이 어떠셨나요? 평가해주시면 이후 개선에 참고하겠습니다! :)</div>
      <div class="stars-outer">
        <i class="far fa-star" title="1 Star" data-behavior="tutorial-rating" data-count="1"></i>
        <i class="far fa-star" title="2 Stars" data-behavior="tutorial-rating" data-count="2"></i>
        <i class="far fa-star" title="3 Stars" data-behavior="tutorial-rating" data-count="3"></i>
        <i class="far fa-star" title="4 Stars" data-behavior="tutorial-rating" data-count="4"></i>
        <i class="far fa-star" title="5 Stars" data-behavior="tutorial-rating" data-count="5"></i>
      </div>
    </div>
    <hr class="rating-hr hr-bottom"/>
  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2024, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorch Korea User Group).

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>
if((window.location.href.indexOf("/prototype/")!= -1) && (window.location.href.indexOf("/prototype/prototype_index")< 1))
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-flask" aria-hidden="true">&nbsp</i> 이 튜토리얼은 프로토타입(prototype) 기능들에 대해서 설명하고 있습니다. 프로토타입 기능은 일반적으로 피드백 및 테스트용으로, 런타임 플래그 없이는 PyPI나 Conda로 배포되는 바이너리에서는 사용할 수 없습니다.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Pytorch Tensor 소개</a><ul>
<li><a class="reference internal" href="#tensor">Tensor 생성하기</a><ul>
<li><a class="reference internal" href="#tensor-seed">무작위 Tensor와 Seed 사용하기</a></li>
<li><a class="reference internal" href="#tensor-shape">Tensor의 shape</a></li>
<li><a class="reference internal" href="#id2">Tensor 자료형</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id3">PyTorch Tensor에서 산술 &amp; 논리 연산</a><ul>
<li><a class="reference internal" href="#tensor-broadcasting">개요: Tensor Broadcasting</a></li>
<li><a class="reference internal" href="#id4">Tensor를 사용하는 다양한 연산들</a></li>
<li><a class="reference internal" href="#id5">Tensor의 값을 변경하기</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id6">Tensor를 복사하기</a></li>
<li><a class="reference internal" href="#gpu">GPU 환경으로 이동하기</a></li>
<li><a class="reference internal" href="#id7">Tensor의 shape 다루기</a><ul>
<li><a class="reference internal" href="#id8">차원의 개수 변경하기</a></li>
</ul>
</li>
<li><a class="reference internal" href="#numpy">NumPy로 변환</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  
  
     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
         <script src="../../_static/translations.js"></script>
         <script src="../../_static/katex.min.js"></script>
         <script src="../../_static/auto-render.min.js"></script>
         <script src="../../_static/katex_autorenderer.js"></script>
         <script src="../../_static/design-tabs.js"></script>
     
  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
//add microsoft link
if(window.location.href.indexOf("/beginner/basics/")!= -1)
{
  var url="https://docs.microsoft.com/learn/paths/pytorch-fundamentals/?wt.mc_id=aiml-7486-cxa";
  switch(window.location.pathname.split("/").pop().replace('.html',''))
  {
    case"quickstart_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/9-quickstart?WT.mc_id=aiml-7486-cxa";
      break;
    case"tensorqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/2-tensors?WT.mc_id=aiml-7486-cxa";
      break;
    case"data_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/3-data?WT.mc_id=aiml-7486-cxa";
      break;
    case"transforms_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/4-transforms?WT.mc_id=aiml-7486-cxa";
      break;
    case"buildmodel_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/5-model?WT.mc_id=aiml-7486-cxa";
      break;
    case"autogradqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/6-autograd?WT.mc_id=aiml-7486-cxa";
      break;
    case"optimization_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/7-optimization?WT.mc_id=aiml-7486-cxa";
      break;
    case"saveloadrun_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/8-inference?WT.mc_id=aiml-7486-cxa";
    }

    $(".pytorch-call-to-action-links").children().first().before("<a href="+url+' data-behavior="call-to-action-event" data-response="Run in Microsoft Learn" target="_blank"><div id="microsoft-learn-link" style="padding-bottom: 0.625rem;border-bottom: 1px solid #f3f4f7;padding-right: 2.5rem;display: -webkit-box;  display: -ms-flexbox; display: flex; -webkit-box-align: center;-ms-flex-align: center;align-items: center;"><img class="call-to-action-img" src="../../_static/images/microsoft-logo.svg"/><div class="call-to-action-desktop-view">Run in Microsoft Learn</div><div class="call-to-action-mobile-view">Learn</div></div></a>')
  }
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-LZRD6GXDLF"></script>
<script data-cfasync="false">
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-LZRD6GXDLF');   // GA4
  gtag('config', 'UA-71919972-3');  // UA
</script>


<script data-cfasync="false">
  $("[data-behavior='call-to-action-event']").on('click', function(){
    ga('send', {
      hitType: 'event',
      eventCategory: $(this).attr("data-response"),
      eventAction: 'click',
      eventLabel: window.location.href
    });

    gtag('event', 'click', {
      'event_category': $(this).attr("data-response"),
      'event_label': $("h1").first().text(),
      'tutorial_link': window.location.href
    });
   });

   $("[data-behavior='tutorial-rating']").on('click', function(){
    gtag('event', 'click', {
      'event_category': 'Tutorial Rating',
      'event_label': $("h1").first().text(),
      'value': $(this).attr("data-count"),
      'customEvent:Rating': $(this).attr("data-count") // send to GA custom dimension customEvent:Rating.
    });
   });

   if (location.pathname == "/") {
     $(".rating-container").hide();
     $(".hr-bottom").hide();
   }
</script>

<script type="text/javascript">
  var collapsedSections = ['파이토치(PyTorch) 레시피', '파이토치(PyTorch) 배우기', 'Introduction to PyTorch on YouTube', '이미지/비디오', '오디오', '텍스트', '백엔드', '강화학습', 'PyTorch 모델을 프로덕션 환경에 배포하기', 'PyTorch 프로파일링', 'Code Transforms with FX', '프론트엔드 API', 'PyTorch 확장하기', '모델 최적화', '병렬 및 분산 학습', 'Edge with ExecuTorch', '추천 시스템', 'Multimodality'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>PyTorchKorea @ GitHub</h2>
          <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
          <a class="with-right-arrow" href="https://github.com/PyTorchKorea" target="_blank">GitHub로 이동</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>한국어 튜토리얼</h2>
          <p>한국어로 번역 중인 PyTorch 튜토리얼입니다.</p>
          <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>커뮤니티</h2>
          <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
          <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.kr/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
            <li><a href="https://pytorch.kr//about">사용자 모임 소개</a></li>
            <li><a href="https://pytorch.kr//about/contributors">기여해주신 분들</a></li>
            <li><a href="https://pytorch.kr//resources">리소스</a></li>
            <li><a href="https://pytorch.kr//coc">행동 강령</a></li>
          </ul>
        </div>
      </div>
      <div class="trademark-disclaimer">
        <ul>
          <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 정책은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
        </ul>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.kr/get-started">시작하기</a>
          </li>

          <li class="active">
            <a href="https://tutorials.pytorch.kr/">튜토리얼</a>
          </li>

          <li>
            <a href="https://pytorch.kr/hub">허브</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">커뮤니티</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>