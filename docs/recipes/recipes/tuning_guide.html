


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="ko" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="ko" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta property="og:title" content="성능 튜닝 가이드" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tutorials.pytorch.kr/recipes/recipes/tuning_guide.html" />
<meta property="og:site_name" content="PyTorch Tutorials KR" />
<meta property="og:description" content="저자: Szymon Migacz 역자: 오왕택 성능 튜닝 가이드는 PyTorch에서 딥러닝 모델의 학습이나 추론 속도를 향상시킬 수 있는 최적화 기법과 같은 좋은 예시를 소개합니다. 제시된 기법은 몇 줄의 코드만 변경해서 구현 가능하며, 모든 도메인의 다양한 딥러닝 모델에 적용할 수 있습니다. 일반적인 최적화 기법: 비동기식으로 데이터 가져오기 및 데이터 증강법: torch.utils.data.DataLoader 는 각각 워커의 subprocess에서 비동기식 데이터 로딩과 데이터 증강을 지원합니다. DataLoader 의 n..." />
<meta property="og:image" content="https://tutorials.pytorch.kr/_static/logos/logo-kr-sm-dark.png" />
<meta property="og:image:alt" content="PyTorch Tutorials KR" />
<meta name="description" content="저자: Szymon Migacz 역자: 오왕택 성능 튜닝 가이드는 PyTorch에서 딥러닝 모델의 학습이나 추론 속도를 향상시킬 수 있는 최적화 기법과 같은 좋은 예시를 소개합니다. 제시된 기법은 몇 줄의 코드만 변경해서 구현 가능하며, 모든 도메인의 다양한 딥러닝 모델에 적용할 수 있습니다. 일반적인 최적화 기법: 비동기식으로 데이터 가져오기 및 데이터 증강법: torch.utils.data.DataLoader 는 각각 워커의 subprocess에서 비동기식 데이터 로딩과 데이터 증강을 지원합니다. DataLoader 의 n..." />
<meta property="og:ignore_canonical" content="true" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>성능 튜닝 가이드 &mdash; 파이토치 한국어 튜토리얼 (PyTorch tutorials in Korean)</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.22/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/custom2.css" type="text/css" />
    <link rel="index" title="색인" href="../../genindex.html" />
    <link rel="search" title="검색" href="../../search.html" />
    <link rel="next" title="(beta) Compiling the optimizer with torch.compile" href="../compiling_optimizer.html" />
    <link rel="prev" title="자동 혼합 정밀도(Automatic Mixed Precision) 가이드" href="amp_recipe.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css" integrity="sha384-vSIIfh2YWi9wW0r9iZe7RJPrKwp6bG+s9QZMoITbCckVJqGCCRhc+ccxNcdpHuYu" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.kr/get-started">시작하기</a>
          </li>

          <li class="active">
            <a href="https://tutorials.pytorch.kr/">튜토리얼</a>
          </li>

          <li>
            <a href="https://pytorch.kr/hub">허브</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">커뮤니티</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  2.3.1
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Tutorials" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">파이토치(PyTorch) 레시피</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../recipes_index.html">모든 레시피 보기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../prototype/prototype_index.html">모든 프로토타입 레시피 보기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">파이토치(PyTorch) 시작하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/intro.html">파이토치(PyTorch) 기본 익히기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/quickstart_tutorial.html">빠른 시작(Quickstart)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/tensorqs_tutorial.html">텐서(Tensor)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/data_tutorial.html">Dataset과 DataLoader</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/transforms_tutorial.html">변형(Transform)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/buildmodel_tutorial.html">신경망 모델 구성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/autogradqs_tutorial.html"><code class="docutils literal notranslate"><span class="pre">torch.autograd</span></code>를 사용한 자동 미분</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/optimization_tutorial.html">모델 매개변수 최적화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/basics/saveloadrun_tutorial.html">모델 저장하고 불러오기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Introduction to PyTorch on YouTube</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt.html">PyTorch 소개 - YouTube 시리즈</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/introyt1_tutorial.html">PyTorch 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/tensors_deeper_tutorial.html">Pytorch Tensor 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/autogradyt_tutorial.html">The Fundamentals of Autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/modelsyt_tutorial.html">Building Models with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/tensorboardyt_tutorial.html">PyTorch TensorBoard 지원</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/trainingyt.html">Training with PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/introyt/captumyt.html">Model Understanding with Captum</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">파이토치(PyTorch) 배우기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/deep_learning_60min_blitz.html">PyTorch로 딥러닝하기: 60분만에 끝장내기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/pytorch_with_examples.html">예제로 배우는 파이토치(PyTorch)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/nn_tutorial.html"><cite>torch.nn</cite> 이 <em>실제로</em> 무엇인가요?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tensorboard_tutorial.html">TensorBoard로 모델, 데이터, 학습 시각화하기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">이미지/비디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchvision_tutorial.html">TorchVision Object Detection Finetuning Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/transfer_learning_tutorial.html">컴퓨터 비전(Vision)을 위한 전이학습(Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/fgsm_tutorial.html">적대적 예제 생성(Adversarial Example Generation)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dcgan_faces_tutorial.html">DCGAN 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/vt_tutorial.html">배포를 위해 비전 트랜스포머(Vision Transformer) 모델 최적화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tiatoolbox_tutorial.html">Pytorch와 TIAToolbox를 사용한 전체 슬라이드 이미지(Whole Slide Image) 분류</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">오디오</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_io_tutorial.html">Audio I/O</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_resampling_tutorial.html">Audio Resampling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_data_augmentation_tutorial.html">Audio Data Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_feature_extractions_tutorial.html">Audio Feature Extractions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_feature_augmentation_tutorial.html">Audio Feature Augmentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/audio_datasets_tutorial.html">오디오 데이터셋</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/speech_recognition_pipeline_tutorial.html">Speech Recognition with Wav2Vec2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/text_to_speech_with_torchaudio.html">Text-to-speech with Tacotron2</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/forced_alignment_with_torchaudio_tutorial.html">wav2vec2을 이용한 강제 정렬</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">텍스트</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/bettertransformer_tutorial.html">Better Transformer를 이용한 고속 트랜스포머 추론</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_classification_tutorial.html">기초부터 시작하는 NLP: 문자-단위 RNN으로 이름 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/char_rnn_generation_tutorial.html">기초부터 시작하는 NLP:  문자-단위 RNN으로 이름 생성하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/seq2seq_translation_tutorial.html">기초부터 시작하는 NLP: Sequence to Sequence 네트워크와 Attention을 이용한 번역</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/text_sentiment_ngrams_tutorial.html">torchtext 라이브러리로 텍스트 분류하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/translation_transformer.html"><code class="docutils literal notranslate"><span class="pre">nn.Transformer</span></code> 와 torchtext로 언어 번역하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/torchtext_custom_dataset_tutorial.html">TorchText를 사용하여 사용자 정의 글 데이터셋 전처리하기</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">백엔드</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">강화학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_q_learning.html">강화 학습 (DQN) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/reinforcement_ppo.html">Reinforcement Learning (PPO) with TorchRL Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/mario_rl_tutorial.html">마리오 게임 RL 에이전트로 학습하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/pendulum.html">Pendulum: Writing your environment and transforms with TorchRL</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch 모델을 프로덕션 환경에 배포하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/onnx/intro_onnx.html">Introduction to ONNX</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/flask_rest_api_tutorial.html">Flask를 사용하여 Python에서 PyTorch를 REST API로 배포하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/Intro_to_TorchScript_tutorial.html">TorchScript 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_export.html">C++에서 TorchScript 모델 로딩하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/super_resolution_with_onnxruntime.html">(선택) PyTorch 모델을 ONNX으로 변환하고 ONNX 런타임에서 실행하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/realtime_rpi.html">Raspberry Pi 4 에서 실시간 추론(Inference) (30fps!)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch 프로파일링</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/profiler.html">PyTorch 모듈 프로파일링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/hta_intro_tutorial.html">전체론적(Holistic Trace Analysis) 추적 분석 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/hta_trace_diff_tutorial.html">종합적 분석을 이용한 트레이스 차이 분석</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Code Transforms with FX</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/fx_conv_bn_fuser.html">(베타) FX에서 합성곱/배치 정규화(Convolution/Batch Norm) 결합기(Fuser) 만들기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/fx_profiling_tutorial.html">(beta) Building a Simple CPU Performance Profiler with FX</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">프론트엔드 API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/memory_format_tutorial.html">(베타) PyTorch를 사용한 Channels Last 메모리 형식</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/forward_ad_usage.html">Forward-mode Automatic Differentiation (Beta)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/jacobians_hessians.html">Jacobians, Hessians, hvp, vhp, and more: composing function transforms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ensembling.html">모델 앙상블</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/per_sample_grads.html">Per-sample-gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_frontend.html">PyTorch C++ 프론트엔드 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch-script-parallelism.html">TorchScript의 동적 병렬 처리(Dynamic Parallelism)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_autograd.html">C++ 프론트엔드의 자동 미분 (autograd)</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PyTorch 확장하기</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/custom_function_double_backward_tutorial.html">Double Backward with Custom Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/custom_function_conv_bn_tutorial.html">Fusing Convolution and Batch Norm using Custom Function</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/cpp_extension.html">Custom C++ and CUDA Extensions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/torch_script_custom_classes.html">커스텀 C++ 클래스로 TorchScript 확장하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/dispatcher.html">Registering a Dispatched Operator in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/extend_dispatcher.html">Extending dispatcher for a new backend in C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/privateuseone.html">Facilitating New Backend Integration by PrivateUse1</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">모델 최적화</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/profiler.html">PyTorch 모듈 프로파일링하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/tensorboard_profiler_tutorial.html">텐서보드를 이용한 파이토치 프로파일러</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/hyperparameter_tuning_tutorial.html">Ray Tune을 사용한 하이퍼파라미터 튜닝</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/vt_tutorial.html">배포를 위해 비전 트랜스포머(Vision Transformer) 모델 최적화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/parametrizations.html">Parametrizations Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/pruning_tutorial.html">가지치기 기법(Pruning) 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/dynamic_quantization_tutorial.html">(베타) LSTM 기반 단어 단위 언어 모델의 동적 양자화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dynamic_quantization_bert_tutorial.html">(베타) BERT 모델 동적 양자화하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/quantized_transfer_learning_tutorial.html">(베타) 컴퓨터 비전 튜토리얼을 위한 양자화된 전이학습(Quantized Transfer Learning)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/static_quantization_tutorial.html">(베타) PyTorch에서 Eager Mode를 이용한 정적 양자화</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchserve_with_ipex.html">Grokking PyTorch Intel CPU performance from first principles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchserve_with_ipex_2.html">Grokking PyTorch Intel CPU performance from first principles (Part 2)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/nvfuser_intro_tutorial.html">Getting Started - Accelerate Your Scripts with nvFuser</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ax_multiobjective_nas_tutorial.html">Multi-Objective NAS with Ax</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torch_compile_tutorial.html">Introduction to <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/inductor_debug_cpu.html">Inductor CPU backend debugging and profiling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html">(Beta) Scaled Dot Product Attention (SDPA)로 고성능 트랜스포머(Transformers) 구현하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#torch-compile-sdpa"><code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> 과 함께 SDPA 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#sdpa-atteition-bias">SDPA를 <code class="docutils literal notranslate"><span class="pre">atteition.bias</span></code> 하위 클래스와 사용하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/scaled_dot_product_attention_tutorial.html#id8">결론</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/knowledge_distillation_tutorial.html">Knowledge Distillation Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">병렬 및 분산 학습</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../distributed/home.html">Distributed and Parallel Training Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/dist_overview.html">PyTorch Distributed Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/ddp_series_intro.html">PyTorch의 분산 데이터 병렬 처리 - 비디오 튜토리얼</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/model_parallel_tutorial.html">단일 머신을 사용한 모델 병렬화 모범 사례</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/ddp_tutorial.html">분산 데이터 병렬 처리 시작하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dist_tuto.html">PyTorch로 분산 어플리케이션 개발하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/FSDP_tutorial.html">Fully Sharded Data Parallel(FSDP) 시작하기</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/FSDP_adavnced_tutorial.html">Advanced Model Training with Fully Sharded Data Parallel (FSDP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/TP_tutorial.html">Large Scale Transformer model training with Tensor Parallel (TP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/process_group_cpp_extension_tutorial.html">Cpp 확장을 사용한 프로세스 그룹 백엔드 사용자 정의</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_tutorial.html">Getting Started with Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_param_server_tutorial.html">Implementing a Parameter Server Using Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/dist_pipeline_parallel_tutorial.html">Distributed Pipeline Parallelism Using RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/rpc_async_execution.html">Implementing Batch RPC Processing Using Asynchronous Executions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/rpc_ddp_tutorial.html">분산 데이터 병렬(DDP)과 분산 RPC 프레임워크 결합</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/ddp_pipeline.html">분산 데이터 병렬 처리와 병렬 처리 파이프라인을 사용한 트랜스포머 모델 학습</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/generic_join.html">Distributed Training with Uneven Inputs Using the Join Context Manager</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Edge with ExecuTorch</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/tutorials/export-to-executorch-tutorial.html">Exporting to ExecuTorch Tutorial</a></li>
<li class="toctree-l1"><a class="reference external" href=" https://pytorch.org/executorch/stable/running-a-model-cpp-tutorial.html">Running an ExecuTorch Model in C++ Tutorial</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/tutorials/sdk-integration-tutorial.html">Using the ExecuTorch SDK to Profile a Model</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/demo-apps-ios.html">Building an ExecuTorch iOS Demo App</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/demo-apps-android.html">Building an ExecuTorch Android Demo App</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/executorch/stable/examples-end-to-end-to-lower-model-to-delegate.html">Lowering a Model as a Delegate</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">추천 시스템</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../intermediate/torchrec_tutorial.html">TorchRec 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../advanced/sharding.html">Exploring TorchRec sharding</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Multimodality</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../beginner/flava_finetuning_tutorial.html">TorchMultimodal 튜토리얼: FLAVA 미세조정</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Tutorials
          
        </a> &gt;
      </li>

        
          <li><a href="../recipes_index.html">파이토치 레시피</a> &gt;</li>
        
      <li>성능 튜닝 가이드</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../../_sources/recipes/recipes/tuning_guide.rst.txt" rel="nofollow"><img src="../../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        

          <div class="pytorch-call-to-action-links">
            <div id="tutorial-type">recipes/recipes/tuning_guide</div>

            <div id="google-colab-link">
              <img class="call-to-action-img" src="../../_static/images/pytorch-colab.svg"/>
              <div class="call-to-action-desktop-view">Run in Google Colab</div>
              <div class="call-to-action-mobile-view">Colab</div>
            </div>
            <div id="download-notebook-link">
              <img class="call-to-action-notebook-img" src="../../_static/images/pytorch-download.svg"/>
              <div class="call-to-action-desktop-view">Download Notebook</div>
              <div class="call-to-action-mobile-view">Notebook</div>
            </div>
            <div id="github-view-link">
              <img class="call-to-action-img" src="../../_static/images/pytorch-github.svg"/>
              <div class="call-to-action-desktop-view">View on GitHub</div>
              <div class="call-to-action-mobile-view">GitHub</div>
            </div>
          </div>

        

          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">참고</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-recipes-recipes-tuning-guide-py"><span class="std std-ref">here</span></a>
to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="sphx-glr-recipes-recipes-tuning-guide-py">
<span id="id1"></span><h1>성능 튜닝 가이드<a class="headerlink" href="#sphx-glr-recipes-recipes-tuning-guide-py" title="이 제목에 대한 퍼머링크">¶</a></h1>
<p><strong>저자</strong>: <a class="reference external" href="https://github.com/szmigacz">Szymon Migacz</a>
<strong>역자</strong>: <a class="reference external" href="https://github.com/ohkingtaek">오왕택</a></p>
<p>성능 튜닝 가이드는 PyTorch에서 딥러닝 모델의 학습이나 추론 속도를 향상시킬 수 있는 최적화 기법과 같은 좋은 예시를 소개합니다.
제시된 기법은 몇 줄의 코드만 변경해서 구현 가능하며, 모든 도메인의 다양한 딥러닝 모델에 적용할 수 있습니다.</p>
<div class="section" id="id3">
<h2>일반적인 최적화 기법<a class="headerlink" href="#id3" title="이 제목에 대한 퍼머링크">¶</a></h2>
<div class="section" id="id4">
<h3>비동기식으로 데이터 가져오기 및 데이터 증강법<a class="headerlink" href="#id4" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader">torch.utils.data.DataLoader</a>
는 각각 워커의 subprocess에서 비동기식 데이터 로딩과 데이터 증강을 지원합니다.
<code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 의 num_worker 기본 설정은 <code class="docutils literal notranslate"><span class="pre">num_worker=0</span></code> 으로, 이는 데이터 로딩이
동기적으로 이루어지며 메인 프로세스에서 실행됨을 의미합니다. 결과적으로 메인 학습 프로세스는 데이터를
사용할 수 있을 때까지 기다려야 실행할 수 있습니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">num_workers</span> <span class="pre">&gt;</span> <span class="pre">0</span></code> 으로 설정하면 비동기식 데이터 로딩과 학습과 데이터 로딩의 동시 처리가
가능합니다. <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> 값은 작업량, CPU, GPU, 학습 데이터의 위치에 따라 조정해야
합니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 는 <code class="docutils literal notranslate"><span class="pre">pin_memory</span></code> 인자를 받으며 기본값은 <code class="docutils literal notranslate"><span class="pre">False</span></code> 입니다. GPU를
사용하는 경우 <code class="docutils literal notranslate"><span class="pre">pin_memory=True</span></code> 로 설정하는 것이 좋습니다. 이는 <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> 가
고정된 메모리를 사용하고, 호스트에서 GPU로 더 빠르고 비동기적인 메모리 복사합니다.</p>
</div>
<div class="section" id="id5">
<h3>검증 및 추론 시 변화도 계산 비활성화하는 방법<a class="headerlink" href="#id5" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>PyTorch는 변화도가 필요한 Tensor와 관련된 모든 연산의 중간 버퍼를 저장합니다. 하지만 일반적으로
검증이나 추론 단계에서는 변화도가 필요하지 않습니다.
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad">torch.no_grad()</a>
컨텍스트 관리자를 사용하여 특정 코드 블록 내에서 변화도 계산을 비활성화할 수 있습니다.
이를 통해 실행 속도가 빨라지고 필요한 메모리 양이 줄어듭니다.
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad">torch.no_grad()</a>
는 함수 데코레이터로도 사용할 수 있습니다.</p>
</div>
<div class="section" id="id7">
<h3>합성곱 계층 이후에 바로 배치 정규화 계층이 오는 경우에 편향을 비활성화하는 방법<a class="headerlink" href="#id7" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d">torch.nn.Conv2d()</a>
함수에는 기본적으로 <code class="docutils literal notranslate"><span class="pre">bias</span></code> 매개변수가 <code class="docutils literal notranslate"><span class="pre">True</span></code> 로 설정되어 있습니다 (이는
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#torch.nn.Conv1d">Conv1d</a>
및
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html#torch.nn.Conv3d">Conv3d</a>
에서도 동일합니다).</p>
<p><code class="docutils literal notranslate"><span class="pre">nn.Conv2d</span></code> 계층 바로 뒤에 <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm2d</span></code> 계층이 이어진다면 합성곱 계층에서
편향은 필요하지 않으므로 <code class="docutils literal notranslate"><span class="pre">nn.Conv2d(...,</span> <span class="pre">bias=False,</span> <span class="pre">...)</span></code> 로 설정하세요.
<code class="docutils literal notranslate"><span class="pre">BatchNorm2d</span></code> 의 첫 단계에서 평균을 빼주기 때문에 필요하지 않으며, 이는 편향의 효과를
상쇄시킵니다.</p>
<p>이 원리는 1차원 및 3차원 합성곱에서도 동일하게 적용되며 <code class="docutils literal notranslate"><span class="pre">BatchNorm</span></code> (또는 다른 정규화
계층)이 합성곱의 편향과 동일한 차원을 정규화할 경우에 해당됩니다.</p>
<p><a class="reference external" href="https://github.com/pytorch/vision">torchvision</a> 에서 제공하는 모델은
이미 이 최적화를 구현하고 있습니다.</p>
</div>
<div class="section" id="model-zero-grad-optimizer-zero-grad-parameter-grad-none">
<h3>model.zero_grad()나 optimizer.zero_grad() 대신 parameter.grad = None 사용하는 방법<a class="headerlink" href="#model-zero-grad-optimizer-zero-grad-parameter-grad-none" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>다음과 같은 방식으로 변화도를 초기화하는 대신:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="c1"># 또는</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
</pre></div>
</div>
<p>아래와 같은 방법을 대신 사용하세요:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">param</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
</pre></div>
</div>
<p>두 번째 코드는 각 개별 매개변수의 메모리를 0으로 초기화하지 않으며,
이후의 역전파 과정에서 변화도를 저장할 때 더하기 대신 대입 연산을 사용하여 메모리 연산 수를 줄입니다.</p>
<p>변화도를 0으로 설정하는 것과 <code class="docutils literal notranslate"><span class="pre">None</span></code> 으로 설정하는 것은 약간의 수치적 차이가 있으므로 자세한
내용은
<a class="reference external" href="https://pytorch.org/docs/master/optim.html#torch.optim.Optimizer.zero_grad">torch.optim</a>
를 참조하세요.</p>
<p>또는 PyTorch 1.7부터 <code class="docutils literal notranslate"><span class="pre">model</span></code> 이나 <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad(set_to_none=True)</span></code>
를 호출할 수 있습니다.</p>
</div>
<div class="section" id="id8">
<h3>연산을 결합하여 최적화하는 방법<a class="headerlink" href="#id8" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>행렬에서 element-wise 덧셈, 곱셈 같은 연산과 <cite>sin()</cite> , <cite>cos()</cite> , <cite>sigmoid()</cite> 같은 수학
함수 등의 point-wise 연산들은 하나의 커널로 결합할 수 있습니다. 이러한 결합은 메모리 접근과 커널
실행 시간을 줄이는 데 도움이 됩니다. 일반적으로 point-wise 연산은 메모리에 바인딩됩니다.
PyTorch의 eager-mode에서는 각 연산마다 커널을 실행하므로 메모리에서 데이터를 불러와 연산을
수행하고 (종종 가장 시간이 적게 걸리는 단계) 결과를 다시 메모리에 쓰는 과정이 필요합니다.</p>
<p>결합된 연산자를 사용하면 여러 point-wise 연산을 위해 단 하나의 커널만 실행되고, 데이터는 한
번만 불러오고 저장됩니다. 특히 이러한 효율적인 방법은 활성화 함수, 옵티마이저, 직접 수정한 RNN 셀
등에서 유용합니다.</p>
<p>PyTorch 2에서는 TorchInductor라는 컴파일러를 통해 자동으로 커널을 결합하는 compile-mode를
도입했습니다. TorchInductor는 단순한 element-wise 연산뿐만 아니라 최적의 성능을 위해
point-wise 연산과 축소(reduction) 연산을 고급 결합할 수 있는 기능을 제공하여 성능을
최적화합니다.</p>
<p>가장 간단한 경우, 연산 결합은 함수 정의에
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.compile.html">torch.compile</a>
데코레이터를 적용하여 활성화할 수 있습니다. 예시:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@torch</span><span class="o">.</span><span class="n">compile</span>
<span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">erf</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="mf">1.41421</span><span class="p">))</span>
</pre></div>
</div>
<p>고급 사용 사례에 대해서는 <a class="reference external" href="https://tutorials.pytorch.kr/intermediate/torch_compile_tutorial.html">Introduction to torch.compile</a>
를 참조하세요.</p>
</div>
<div class="section" id="channels-last">
<h3>컴퓨터 비전 모델에 대해 channels_last 메모리 형식 활성화하는 방법<a class="headerlink" href="#channels-last" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>PyTorch 1.5에서는 합성곱 신경망에 대해 channels_last 메모리 형식을 지원하기 시작했습니다.
이 포맷은 <a class="reference external" href="https://www.nvidia.com/en-us/data-center/tensor-cores/">Tensor Cores</a>
를 사용하여 합성곱 신경망을 더욱 가속화하기 위해
<a class="reference external" href="https://pytorch.org/docs/stable/amp.html">AMP</a>
와 함께 사용할 수 있도록 설계되었습니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">channels_last</span></code> 기능은 아직 실험 단계에 있지만 표준 컴퓨터 비전 모델(예시: ResNet-50,
SSD)에서는 동작할 것으로 예상됩니다. 모델을 <code class="docutils literal notranslate"><span class="pre">channels_last</span></code> 형식으로 변환하는 방법에 대해서는
<a class="reference external" href="https://tutorials.pytorch.kr/intermediate/memory_format_tutorial.html">(베타) PyTorch를 사용한 Channels Last 메모리 형식</a>
을 참조하세요. 튜토리얼에는
<a class="reference external" href="https://tutorials.pytorch.kr/intermediate/memory_format_tutorial.html#converting-existing-models">기존 모델들 변환하기</a>
섹션이 포함되어 있습니다.</p>
</div>
<div class="section" id="id10">
<h3>중간 버퍼를 체크포인트로 만드는 방법<a class="headerlink" href="#id10" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>버퍼 체크포인트 저장은 모델 학습 중 메모리 용량 부담을 완화하기 위한 기법입니다. 역전파에서 앞부분의
변화도를 계산하기 위해 모든 계층의 입력을 저장하는 대신, 일부 계층의 입력만 저장하고 나머지는
역전파 중에 재계산합니다. 메모리 요구 사항이 줄어들어 배치 크기를 증가시킬 수 있으며, 이는 활용
효율을 개선할 수 있습니다.</p>
<p>체크포인트 저장할 대상은 신중하게 선택해야 합니다. 가장 좋은 방법은 재계산 비용이 적은 대규모
레이어의 출력을 저장하지 않는 것입니다. 예를 들어, 활성화 함수(예시: <code class="docutils literal notranslate"><span class="pre">ReLU</span></code> , <code class="docutils literal notranslate"><span class="pre">Sigmoid</span></code>
, <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> ), up/down 샘플링, 작은 누적 깊이(accumulation depth)를 가진 행렬-벡터 연산
등이 체크포인트 저장 대상으로 적합합니다.</p>
<p>PyTorch는 자동으로 체크포인트 저장 및 재계산을 수행하는
<a class="reference external" href="https://pytorch.org/docs/stable/checkpoint.html">torch.utils.checkpoint</a>
API를 지원합니다.</p>
</div>
<div class="section" id="api">
<h3>디버깅 API 비활성화<a class="headerlink" href="#api" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>많은 PyTorch API는 디버깅을 위해 설계되었으며 정규 학습 실행 시에는 비활성화해야 합니다.</p>
<ul class="simple">
<li><p>이상탐지 (anomaly detection):
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.detect_anomaly">torch.autograd.detect_anomaly</a>
또는
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.set_detect_anomaly">torch.autograd.set_detect_anomaly(True)</a></p></li>
<li><p>profiler 관련:
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.emit_nvtx">torch.autograd.profiler.emit_nvtx</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile">torch.autograd.profiler.profile</a></p></li>
<li><p>autograd <code class="docutils literal notranslate"><span class="pre">gradcheck</span></code>:
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradcheck">torch.autograd.gradcheck</a>
또는
<a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.gradgradcheck">torch.autograd.gradgradcheck</a></p></li>
</ul>
</div>
</div>
<div class="section" id="cpu">
<h2>CPU 관련 최적화<a class="headerlink" href="#cpu" title="이 제목에 대한 퍼머링크">¶</a></h2>
<div class="section" id="numa">
<h3>비균일 메모리 접근(NUMA) 제어 활용 방법<a class="headerlink" href="#numa" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>NUMA(비균일 메모리 접근)는 다중 메모리 컨트롤러와 블록이 있는 멀티 소켓 머신에서 메모리의 지역성을
활용하기 위해 데이터 센터 머신에서 사용되는 메모리 레이아웃 디자인입니다. 일반적으로 딥러닝 작업,
학습 또는 추론 모두에서 NUMA 노드 간의 하드웨어 자원 접근 없이 더 나은 성능을 발휘합니다. 따라서
추론은 각 인스턴스가 하나의 소켓에서 실행되도록 여러 인스턴스로 실행할 수 있으며, 이를 통해 처리량을
증가시킬 수 있습니다. 단일 노드에서의 학습 작업에는 분산 학습이 권장되며, 이를 통해 각 학습
프로세스가 하나의 소켓에서 실행되도록 할 수 있습니다.</p>
<p>다음 명령어는 N번째 노드의 코어에서만 PyTorch 스크립트를 실행하며, 소켓 간 메모리
접근을 피하여 메모리 접근 오버헤드를 줄입니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>numactl<span class="w"> </span>--cpunodebind<span class="o">=</span>N<span class="w"> </span>--membind<span class="o">=</span>N<span class="w"> </span>python<span class="w"> </span>&lt;pytorch_script&gt;
</pre></div>
</div>
<p>자세한 설명은
<a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html">여기</a>
서 확인할 수 있습니다.</p>
</div>
<div class="section" id="openmp">
<h3>OpenMP 활용하는 방법<a class="headerlink" href="#openmp" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>OpenMP는 병렬 계산 작업의 성능을 향상시키기 위해 사용됩니다.
<code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> 는 계산 속도를 높이는 가장 간단한 환경 변수입니다. 이는 OpenMP 계산에
사용되는 스레드 수를 결정합니다.
CPU Affinity 설정은 작업이 여러 코어에 분배되는 방식을 제어합니다. 이는 통신 오버헤드와 캐시 라인
무효화 오버헤드 또는 페이지 스레싱 등에 영향을 미치므로 CPU 친화도를 적절히 설정하면 성능 향상이
가능합니다. <code class="docutils literal notranslate"><span class="pre">GOMP_CPU_AFFINITY</span></code> 와 <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> 는 OpenMP 스레드를 물리적 처리
장치에 바인딩하는 방법을 결정합니다. 자세한 정보는
<a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html">여기</a>
에서 확인할 수 있습니다.</p>
<p>다음 명령어를 사용하면 PyTorch가 N개의 OpenMP 스레드에서 작업을 실행합니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span>N
</pre></div>
</div>
<p>일반적으로 GNU OpenMP 구현에서 CPU 친화도를 설정하기 위해 다음 환경 변수를 사용합니다.
<code class="docutils literal notranslate"><span class="pre">OMP_PROC_BIND</span></code> 는 스레드가 프로세서 간에 이동할 수 있는지 여부를 지정합니다. 이를 CLOSE로
설정하면 OpenMP 스레드가 기본 스레드에 가까운 위치 파티션에 유지됩니다. <code class="docutils literal notranslate"><span class="pre">OMP_SCHEDULE</span></code> 는
OpenMP 스레드가 어떻게 스케줄링 되는지를 결정합니다. <code class="docutils literal notranslate"><span class="pre">GOMP_CPU_AFFINITY</span></code> 는 스레드를 특정
CPU에 바인딩합니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">OMP_SCHEDULE</span><span class="o">=</span>STATIC
<span class="nb">export</span><span class="w"> </span><span class="nv">OMP_PROC_BIND</span><span class="o">=</span>CLOSE
<span class="nb">export</span><span class="w"> </span><span class="nv">GOMP_CPU_AFFINITY</span><span class="o">=</span><span class="s2">&quot;N-M&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="intel-openmp-libiomp">
<h3>Intel OpenMP 런타임 라이브러리(<code class="docutils literal notranslate"><span class="pre">libiomp</span></code>)<a class="headerlink" href="#intel-openmp-libiomp" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>기본적으로 PyTorch는 병렬 계산을 위해 GNU OPENMP(GNU <code class="docutils literal notranslate"><span class="pre">libgomp</span></code>)를 사용합니다. Intel
플랫폼에서는 Intel OpenMP 런타임 라이브러리(<code class="docutils literal notranslate"><span class="pre">libiomp</span></code>)가 OpenMP API 사양 지원을
제공합니다. 이는 <code class="docutils literal notranslate"><span class="pre">libgomp</span></code> 보다 더 나은 성능 이점을 제공할 때도 있습니다. 환경 변수
<code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> 를 사용하여 OpenMP 라이브러리를 <code class="docutils literal notranslate"><span class="pre">libiomp</span></code> 로 전환할 수 있습니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span>&lt;path&gt;/libiomp5.so:<span class="nv">$LD_PRELOAD</span>
</pre></div>
</div>
<p>GNU OpenMP의 CPU 친화도 설정과 유사하게, <code class="docutils literal notranslate"><span class="pre">libiomp</span></code> 에서도 CPU 친화도 설정을 제어하는 환경
변수가 제공됩니다. <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> 는 OpenMP 스레드를 물리적 처리 장치에 바인딩합니다.
<code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code> 은 스레드가 병렬 영역의 실행을 완료한 후 대기해야 하는 시간을 ms 단위로
설정합니다. 대부분의 경우 <code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code> 을 1 또는 0으로 설정하면 좋은 성능을 얻을 수
있습니다. 다음 명령어는 Intel OpenMP 런타임 라이브러리에서의 일반적인 설정입니다.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">KMP_AFFINITY</span><span class="o">=</span><span class="nv">granularity</span><span class="o">=</span>fine,compact,1,0
<span class="nb">export</span><span class="w"> </span><span class="nv">KMP_BLOCKTIME</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
</div>
<div class="section" id="id13">
<h3>메모리 할당자 전환<a class="headerlink" href="#id13" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>딥러닝 작업에서는 기존에 <code class="docutils literal notranslate"><span class="pre">malloc</span></code> 함수보다 메모리를 최대한 재사용할 수 있는 <code class="docutils literal notranslate"><span class="pre">Jemalloc</span></code>
또는 <code class="docutils literal notranslate"><span class="pre">TCMalloc</span></code> 을 사용하면 더 나은 성능을 얻을 수 있습니다.
<a class="reference external" href="https://github.com/jemalloc/jemalloc">Jemalloc</a> 은 일반적인 목적의 <code class="docutils literal notranslate"><span class="pre">malloc</span></code>
을 구현한 것으로, 단편화 방지와 확장 가능한 동시성 지원에 중점을 둡니다.
<a class="reference external" href="https://google.github.io/tcmalloc/overview.html">TCMalloc</a> 은 또한 프로그램
실행 속도를 높이기 위한 몇 가지 최적화를 제공합니다. 그 중 하나는 메모리를 캐시에 보관하여 자주
사용되는 객체에 대한 접근 속도를 높이는 것입니다. 이러한 캐시를 할당 해제 후에도 유지하면 나중에
메모리가 다시 할당될 때 비용이 많이 드는 시스템 호출을 피하는 데 도움이 됩니다. 이들 중 하나를
사용하려면 환경 변수 <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> 를 설정하십시오.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span>&lt;jemalloc.so/tcmalloc.so&gt;:<span class="nv">$LD_PRELOAD</span>
</pre></div>
</div>
</div>
<div class="section" id="torchscript-onednn-graph">
<h3>TorchScript로 추론 시 oneDNN Graph 사용하는 방법<a class="headerlink" href="#torchscript-onednn-graph" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>oneDNN Graph는 추론 성능을 크게 향상시킬 수 있습니다. 이는 합성곱, 행렬 곱셈(matmul)과 같은
연산을 주변 연산과 결합하여 처리합니다. PyTorch 2.0에서는 <code class="docutils literal notranslate"><span class="pre">Float32</span></code> 및 <code class="docutils literal notranslate"><span class="pre">BFloat16</span></code>
데이터 유형에 대해 베타 기능으로 지원됩니다. oneDNN Graph는 모델의 그래프를 받아서 예제 입력의
모양을 고려하여 연산자 결합 후보를 식별합니다. 모델은 예제 입력을 사용하여 JIT-traced 되어야
합니다. 동일한 모양의 입력에 대해 몇 번의 워밍업 후 속도가 향상될 수 있습니다. 아래의 예제
코드는 resnet50에 대한 것이지만 사용자가 원하는 모델에서도 oneDNN Graph를 사용할 수 있습니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># oneDNN Graph를 사용하려면 이 추가 코드 한 줄로 충분합니다.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">enable_onednn_fusion</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>oneDNN Graph API를 사용하려면 Float32 추론 시 단 한 줄의 코드를 추가해야 합니다. oneDNN
Graph를 사용 중이라면 <code class="docutils literal notranslate"><span class="pre">torch.jit.optimize_for_inference</span></code> 호출을 피해야 합니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># sample_input은 예상되는 입력과 동일한 모양이어야 합니다.</span>
<span class="n">sample_input</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)]</span>
<span class="c1"># resnet50 모델을 예시로 사용하지만, 아래 줄은 사용자가 원하는 모델에 맞게 수정할 수 있습니다.</span>
<span class="n">model</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">torchvision</span><span class="o">.</span><span class="n">models</span><span class="p">,</span> <span class="s2">&quot;resnet50&quot;</span><span class="p">)()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1"># sample_input으로 모델을 trace하기</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">sample_input</span><span class="p">)</span>
<span class="c1"># torch.jit.freeze 호출하기</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
</pre></div>
</div>
<p>모델이 sample_input을 사용해 JIT-traced되면 몇 번의 워밍업 후 추론에 사용할 수 있습니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="c1"># 몇 번의 워밍업</span>
    <span class="n">traced_model</span><span class="p">(</span><span class="o">*</span><span class="n">sample_input</span><span class="p">)</span>
    <span class="n">traced_model</span><span class="p">(</span><span class="o">*</span><span class="n">sample_input</span><span class="p">)</span>
    <span class="c1"># 워밍업 실행 후 성능 향상 관찰 가능</span>
    <span class="n">traced_model</span><span class="p">(</span><span class="o">*</span><span class="n">sample_input</span><span class="p">)</span>
</pre></div>
</div>
<p>oneDNN Graph용 JIT fuser는 <code class="docutils literal notranslate"><span class="pre">BFloat16</span></code> 데이터 타입을 사용한 추론도 지원하지만, oneDNN
Graph의 성능 이점은 AVX512_BF16 명령어 세트 아키텍처(ISA)의 머신에서 나타납니다.
다음 코드 예시는 oneDNN Graph를 사용해 <code class="docutils literal notranslate"><span class="pre">BFloat16</span></code> 데이터 타입으로 추론하는 예시입니다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># JIT 모드의 AMP는 기본적으로 활성화되어 있으며 eager 모드와 다르게 작동합니다.</span>
<span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_jit_set_autocast_mode</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(</span><span class="n">cache_enabled</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">):</span>
    <span class="c1"># CNN 기반 비전 모델의 Conv-BatchNorm folding은 AMP를 사용할 때 ``torch.fx.experimental.optimization.fuse``를 통해 동작해야 합니다.</span>
    <span class="kn">import</span> <span class="nn">torch.fx.experimental.optimization</span> <span class="k">as</span> <span class="nn">optimization</span>
    <span class="c1"># AMP를 사용하지 않는 경우 optimization.fuse를 호출할 필요는 없습니다.</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">optimization</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">(</span><span class="n">example_input</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="c1"># 몇 번의 워밍업</span>
    <span class="n">model</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>
    <span class="n">model</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>
    <span class="c1"># 워밍업 실행 후 성능 향상 관찰 가능</span>
    <span class="n">model</span><span class="p">(</span><span class="n">example_input</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="pytorch-distributeddataparallel-ddp-cpu">
<h3>PyTorch <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> (DDP) 기능을 사용해 CPU에서 모델 학습하는 방법<a class="headerlink" href="#pytorch-distributeddataparallel-ddp-cpu" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>DLRM과 같은 소규모 모델 또는 메모리에 바인딩 된 모델의 경우 CPU에서 학습하는 것도 좋은 선택입니다.
여러 소켓을 가진 머신에서는, 분산 학습으로 고효율의 하드웨어 자원을 사용하여 학습 과정을 가속할 수 있습니다.
<a class="reference external" href="https://github.com/intel/torch-ccl">Torch-ccl</a> 은 Intel(R)의 <code class="docutils literal notranslate"><span class="pre">oneCCL</span></code>
(집합 통신 라이브러리)로 최적화되어 효율적인 분산 딥러닝 학습을 위해 <code class="docutils literal notranslate"><span class="pre">allreduce</span></code> ,
<code class="docutils literal notranslate"><span class="pre">allgather</span></code> , <code class="docutils literal notranslate"><span class="pre">alltoall</span></code> 과 같은 집합 연산을 구현합니다. Torch-ccl은 PyTorch C10D
<code class="docutils literal notranslate"><span class="pre">ProcessGroup</span></code> API를 구현하며, 외부 <code class="docutils literal notranslate"><span class="pre">ProcessGroup</span></code> 으로 동적으로 로드할 수 있습니다.
PyTorch DDP 모듈에서 구현된 최적화를 통해 <code class="docutils literal notranslate"><span class="pre">torch-ccl</span></code> 은 통신 연산을 가속화합니다. 통신
커널의 최적화 외에도 <code class="docutils literal notranslate"><span class="pre">torch-ccl</span></code> 은 계산과 통신을 동시에 수행하는 기능을 제공합니다.</p>
</div>
</div>
<div class="section" id="gpu">
<h2>GPU 전용 최적화 방법<a class="headerlink" href="#gpu" title="이 제목에 대한 퍼머링크">¶</a></h2>
<div class="section" id="cudnn-auto-tuner">
<h3>cuDNN auto-tuner 활성화하기<a class="headerlink" href="#cudnn-auto-tuner" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p><a class="reference external" href="https://developer.nvidia.com/cudnn">NVIDIA cuDNN</a> 은 합성곱을 계산하기 위해 여러
알고리즘을 지원합니다. Autotuner는 짧은 벤치마크를 실행하고 주어진 하드웨어와 입력 크기에 대해
최상의 성능을 가진 커널을 선택합니다.</p>
<p>합성곱 신경망 (다른 유형은 현재 지원되지 않음)의 경우 학습하기 전에 cuDNN autotuner를
활성화하려면 다음과 같이 설정하십시오:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">benchmark</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<ul class="simple">
<li><p>autotuner의 결정은 비결정적일 수 있습니다. 서로 다른 실행에서 다른 알고리즘이 선택될 수
있습니다. 자세한 내용은
<a class="reference external" href="https://pytorch.org/docs/stable/notes/randomness.html?highlight=determinism">PyTorch: Reproducibility</a>
를 참조하세요.</p></li>
<li><p>드문 상황에서, 예를 들면 입력 크기가 가변적인 경우, 각 입력 크기에 대해 알고리즘 선택과 관련된
오버헤드를 피하기 위해 autotuner를 비활성화하고 합성곱 신경망을 실행하는 것이 더 나을 수 있습니다.</p></li>
</ul>
</div>
<div class="section" id="cpu-gpu">
<h3>불필요한 CPU-GPU 동기화 피하기<a class="headerlink" href="#cpu-gpu" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>CPU가 GPU 같은 가속기보다 최대한 앞서 실행될 수 있도록 불필요한 동기화를 피하여 가속기의 작업 큐에
많은 작업이 포함되도록 하십시오.</p>
<p>가능하면 동기화를 요구하는 작업을 피하십시오. 예시:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">print(cuda_tensor)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuda_tensor.item()</span></code></p></li>
<li><p>메모리 복사 : <code class="docutils literal notranslate"><span class="pre">tensor.cuda()</span></code>,  <code class="docutils literal notranslate"><span class="pre">cuda_tensor.cpu()</span></code> 혹은 이에 상응하는
<code class="docutils literal notranslate"><span class="pre">tensor.to(device)</span></code> 호출</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuda_tensor.nonzero()</span></code></p></li>
<li><p>CUDA tensor에서 수행된 연산 결과에 의존하는 파이썬 제어 흐름
예시: <code class="docutils literal notranslate"><span class="pre">if</span> <span class="pre">(cuda_tensor</span> <span class="pre">!=</span> <span class="pre">0).all()</span></code></p></li>
</ul>
</div>
<div class="section" id="tensor">
<h3>대상 장치에서 직접 tensor 생성하는 방법<a class="headerlink" href="#tensor" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch.rand(size).cuda()</span></code> 를 호출하여 무작위 tensor를 생성하는 대신에 tensor를 직접
장치에서 생성합니다.
<code class="docutils literal notranslate"><span class="pre">torch.rand(size,</span> <span class="pre">device='cuda')</span></code></p>
<p>이는 다음과 같이 <code class="docutils literal notranslate"><span class="pre">device</span></code> 인수를 받아 새로운 tensor를 생성하는 모든 함수에 적용됩니다:
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.rand.html#torch.rand">torch.rand()</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.zeros.html#torch.zeros">torch.zeros()</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.full.html#torch.full">torch.full()</a>
등.</p>
</div>
<div class="section" id="id14">
<h3>혼합 정밀도와 AMP 사용하는 방법<a class="headerlink" href="#id14" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>혼합 정밀도는
<a class="reference external" href="https://www.nvidia.com/en-us/data-center/tensor-cores/">Tensor Cores</a>
를 활용하여 Volta나 최신 GPU 아키텍처에서 최대 3배의 전체 속도 향상을 제공합니다. Tensor
Cores를 사용하려면 AMP를 활성화하고 행렬/텐서 차원이 Tensor Cores를 사용하는 커널 호출
요구 사항을 충족해야 합니다.</p>
<p>Tensor Cores를 사용하려면:</p>
<ul class="simple">
<li><p>size를 8의 배수로 설정 (Tensor Cores의 차원에 맞추기 위해)</p>
<ul>
<li><p><a class="reference external" href="https://docs.nvidia.com/deeplearning/performance/index.html#optimizing-performance">Deep Learning Performance Documentation</a>
에서 자세한 정보와 레이어 유형에 따른 가이드라인을 참조하세요.</p></li>
<li><p>레이어 크기가 고정되지 않고 다른 매개변수에서 유도되는 경우에도 명시적으로 패딩할 수 있습니다.
(예시: NLP 모델의 어휘 크기 등).</p></li>
</ul>
</li>
<li><p>AMP 활성화하기</p>
<ul>
<li><p>혼합 정밀도 학습과 AMP 소개:
<a class="reference external" href="https://www.youtube.com/watch?v=jF4-_ZK_tyc&amp;feature=youtu.be">video</a>,
<a class="reference external" href="https://nvlabs.github.io/eccv2020-mixed-precision-tutorial/files/dusan_stosic-training-neural-networks-with-tensor-cores.pdf">slides</a></p></li>
<li><p>PyTorch 1.6부터 사용할 수 있는 PyTorch AMP:
<a class="reference external" href="https://pytorch.org/docs/stable/amp.html">documentation</a>,
<a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples">examples</a>,
<a class="reference external" href="https://tutorials.pytorch.kr/recipes/recipes/amp_recipe.html">tutorial</a></p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="id16">
<h3>가변 입력 길이에 대비하여 메모리 미리 할당하기<a class="headerlink" href="#id16" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>음성 인식 또는 NLP 모델은 종종 가변 시퀀스 길이를 가진 tensor를 입력으로 학습됩니다. 가변 길이는
PyTorch 캐싱 할당기에서 문제를 일으킬 수 있으며, 성능 저하 또는 예기치 않은 메모리 부족 오류를
초래할 수 있습니다. 짧은 시퀀스 길이의 배치가 더 긴 시퀀스 길이의 배치로 이어지면, PyTorch는
이전 반복의 중간 버퍼를 해제하고 새 버퍼를 재할당해야 합니다. 이 과정은 시간이 많이 소요되며 캐싱
할당기에서 조각화(fragmentation)를 일으켜 메모리 부족 오류를 유발할 수 있습니다.</p>
<p>일반적인 해결 방법은 미리 할당(preallocation)을 구현하는 것입니다.
다음 단계로 구성됩니다:</p>
<ol class="arabic simple">
<li><p>최대 시퀀스 길이(훈련 데이터 세트의 최대 길이 또는 사전 정의된 임계값에 해당)를 갖는 (일반적으로
무작위) 입력 배치를 생성합니다.</p></li>
<li><p>생성된 배치로 순방향 및 역방향 과정을 실행합니다. 옵티마이저나 학습률 스케줄러는 실행하지 않으며,
이 단계는 이후 학습에서 재사용할 수 있는 최대 크기의 버퍼를 미리 할당합니다.</p></li>
<li><p>변화도를 0으로 설정합니다.</p></li>
<li><p>정규 학습을 진행합니다.</p></li>
</ol>
</div>
</div>
<div class="section" id="id17">
<h2>분산 최적화 방법<a class="headerlink" href="#id17" title="이 제목에 대한 퍼머링크">¶</a></h2>
<div class="section" id="id18">
<h3>효율적인 데이터 병렬 백엔드 사용하는 방법<a class="headerlink" href="#id18" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>PyTorch에는 데이터 병렬 학습을 구현하는 두 가지 방법이 있습니다:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.DataParallel.html#torch.nn.DataParallel">torch.nn.DataParallel</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">torch.nn.parallel.DistributedDataParallel</a></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 은 다중 GPU에 대해 훨씬 더 나은 성능과 확장성을 제공합니다.
자세한 정보는 PyTorch 문서의
<a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#use-nn-parallel-distributeddataparallel-instead-of-multiprocessing-or-nn-dataparallel">relevant section of CUDA Best Practices</a>
를 참조하세요.</p>
</div>
<div class="section" id="distributeddataparallel-all-reduce">
<h3>학습할 때 <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 이나 변화도 축적 사용 시 불필요한 all-reduce 건너뛰는 방법<a class="headerlink" href="#distributeddataparallel-all-reduce" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>기본적으로
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">torch.nn.parallel.DistributedDataParallel</a>
은 모든 역전파 과정 후에 변화도 all-reduce를 실행하여 학습에 참여하는 모든 워커에서의 평균 변화도를
계산합니다. 학습 시 변화도 축적을 N단계 동안 사용하는 경우, 모든 학습 단계 후에 all-reduce가
요하지 않습니다. 마지막 역전파 호출 직후, 즉 옵티마이저 실행 직전에만 all-reduce를
수행하면 됩니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 은 특정 반복에 대해 변화도 all-reduce를 비활성화하는
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel.no_sync">no_sync()</a>
컨텍스트 관리자를 제공합니다.
<code class="docutils literal notranslate"><span class="pre">no_sync()</span></code> 는 변화도 축적의 첫 <code class="docutils literal notranslate"><span class="pre">N-1</span></code> 반복에 적용되어야 하며 마지막 반복은 기본 실행을
따르고 필요한 변화도 all-reduce를 수행해야 합니다.</p>
</div>
<div class="section" id="distributeddataparallel-find-unused-parameters-true">
<h3><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel(find_unused_parameters=True)</span></code> 를 사용할 때 생성자와 실행 레이어 순서를 일치시키는 방법<a class="headerlink" href="#distributeddataparallel-find-unused-parameters-true" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">torch.nn.parallel.DistributedDataParallel</a>
은 <code class="docutils literal notranslate"><span class="pre">find_unused_parameters=True</span></code> 와 함께 모델 생성자에서의 레이어와 파라미터 순서를
사용하여 <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 변화도 all-reduce를 위한 버킷을 만듭니다.
<code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 은 all-reduce를 역전파와 겹치게 수행합니다. 특정 버킷에 대한
all-reduce는 주어진 버킷의 모든 파라미터에 대한 변화도가 모두 준비되었을 때 비동기적으로 작동됩니다.</p>
<p>최대로 겹치게 하려면 모델 생성자에서의 순서가 실제 실행 중인 순서와 대략적으로 일치해야 합니다.
순서가 맞지 않으면 전체 버킷에 대한 all-reduce는 마지막으로 도착하는 변화도를 기다리게 되며,
이는 역전파와 all-reduce 간의 겹침을 줄일 수 있고, all-reduce가 노출되어 학습 속도가 느려질 수
있습니다.</p>
<p><code class="docutils literal notranslate"><span class="pre">find_unused_parameters=False</span></code> 가 (기본 설정)인 <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 은
역전파 중에 발견된 연산 순서를 기반으로 자동으로 버킷을 형성합니다.
<code class="docutils literal notranslate"><span class="pre">find_unused_parameters=False</span></code> 를 사용할 때는 최적의 성능을 달성하기 위해 레이어나
파라미터의 순서를 재조정할 필요가 없습니다.</p>
</div>
<div class="section" id="id21">
<h3>분산 설정에서 작업 부하를 분산하는 방법<a class="headerlink" href="#id21" title="이 제목에 대한 퍼머링크">¶</a></h3>
<p>작업 부하 불균형은 일반적으로 순차적인 데이터를 처리하는 모델(예시: 음성 인식, 번역, 언어 모델 등)
에서 발생할 수 있습니다. 하나의 장치가 나머지 장치들보다 긴 시퀀스 길이를 가진 데이터 배치를 받으면,
모든 장치가 마지막으로 작업을 끝내는 워커를 기다리게 됩니다. 역전파는
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">DistributedDataParallel</a>
백엔드와 함께 분산 설정에서 암묵적인 동기화 지점으로 작용합니다.</p>
<p>작업 로드 밸런싱 문제를 해결하는 방법은 여러 가지가 있습니다. 핵심은 각 전역 배치 내에서
모든 워커에 걸쳐 작업 부하를 가능한 한 균일하게 분배하는 것입니다. 예를 들어, Transformer는 배치
내에서 대략 일정한 수의 토큰(변동하는 수의 시퀀스)을 형성하여 불균형을 해결하며, 다른 모델은 유사한
시퀀스 길이를 가진 샘플을 버킷화하거나 데이터셋을 시퀀스 길이에 따라 정렬하여 불균형을 해결합니다.</p>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-recipes-recipes-tuning-guide-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/8c82db84c10318a94cbe213adb618139/tuning_guide.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tuning_guide.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/38991cbc7763ed7e0f1b711da737b391/tuning_guide.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tuning_guide.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../compiling_optimizer.html" class="btn btn-neutral float-right" title="(beta) Compiling the optimizer with torch.compile" accesskey="n" rel="next">Next <img src="../../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="amp_recipe.html" class="btn btn-neutral" title="자동 혼합 정밀도(Automatic Mixed Precision) 가이드" accesskey="p" rel="prev"><img src="../../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  
    <hr class="community-hr hr-top" />
    <div class="community-container">
      <div class="community-prompt">더 궁금하시거나 개선할 내용이 있으신가요? 커뮤니티에 참여해보세요!</div>
      <div class="community-link"><a href="https://discuss.pytorch.kr/" aria-label="PyTorchKoreaCommunity">한국어 커뮤니티 바로가기</a></div>
    </div>
    <hr class="community-hr hr-bottom"/>

    <hr class="rating-hr hr-top" />
    <div class="rating-container">
      <div class="rating-prompt">이 튜토리얼이 어떠셨나요? 평가해주시면 이후 개선에 참고하겠습니다! :)</div>
      <div class="stars-outer">
        <i class="far fa-star" title="1 Star" data-behavior="tutorial-rating" data-count="1"></i>
        <i class="far fa-star" title="2 Stars" data-behavior="tutorial-rating" data-count="2"></i>
        <i class="far fa-star" title="3 Stars" data-behavior="tutorial-rating" data-count="3"></i>
        <i class="far fa-star" title="4 Stars" data-behavior="tutorial-rating" data-count="4"></i>
        <i class="far fa-star" title="5 Stars" data-behavior="tutorial-rating" data-count="5"></i>
      </div>
    </div>
    <hr class="rating-hr hr-bottom"/>
  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2024, PyTorch &amp; 파이토치 한국 사용자 모임(PyTorch Korea User Group).

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
<script>
if((window.location.href.indexOf("/prototype/")!= -1) && (window.location.href.indexOf("/prototype/prototype_index")< 1))
  {
    var div = '<div class="admonition note"><p class="admonition-title">Note</p><p><i class="fa fa-flask" aria-hidden="true">&nbsp</i> 이 튜토리얼은 프로토타입(prototype) 기능들에 대해서 설명하고 있습니다. 프로토타입 기능은 일반적으로 피드백 및 테스트용으로, 런타임 플래그 없이는 PyPI나 Conda로 배포되는 바이너리에서는 사용할 수 없습니다.</p></div>'
    document.getElementById("pytorch-article").insertAdjacentHTML('afterBegin', div)
  }
</script>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">성능 튜닝 가이드</a><ul>
<li><a class="reference internal" href="#id3">일반적인 최적화 기법</a><ul>
<li><a class="reference internal" href="#id4">비동기식으로 데이터 가져오기 및 데이터 증강법</a></li>
<li><a class="reference internal" href="#id5">검증 및 추론 시 변화도 계산 비활성화하는 방법</a></li>
<li><a class="reference internal" href="#id7">합성곱 계층 이후에 바로 배치 정규화 계층이 오는 경우에 편향을 비활성화하는 방법</a></li>
<li><a class="reference internal" href="#model-zero-grad-optimizer-zero-grad-parameter-grad-none">model.zero_grad()나 optimizer.zero_grad() 대신 parameter.grad = None 사용하는 방법</a></li>
<li><a class="reference internal" href="#id8">연산을 결합하여 최적화하는 방법</a></li>
<li><a class="reference internal" href="#channels-last">컴퓨터 비전 모델에 대해 channels_last 메모리 형식 활성화하는 방법</a></li>
<li><a class="reference internal" href="#id10">중간 버퍼를 체크포인트로 만드는 방법</a></li>
<li><a class="reference internal" href="#api">디버깅 API 비활성화</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cpu">CPU 관련 최적화</a><ul>
<li><a class="reference internal" href="#numa">비균일 메모리 접근(NUMA) 제어 활용 방법</a></li>
<li><a class="reference internal" href="#openmp">OpenMP 활용하는 방법</a></li>
<li><a class="reference internal" href="#intel-openmp-libiomp">Intel OpenMP 런타임 라이브러리(<code class="docutils literal notranslate"><span class="pre">libiomp</span></code>)</a></li>
<li><a class="reference internal" href="#id13">메모리 할당자 전환</a></li>
<li><a class="reference internal" href="#torchscript-onednn-graph">TorchScript로 추론 시 oneDNN Graph 사용하는 방법</a></li>
<li><a class="reference internal" href="#pytorch-distributeddataparallel-ddp-cpu">PyTorch <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> (DDP) 기능을 사용해 CPU에서 모델 학습하는 방법</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gpu">GPU 전용 최적화 방법</a><ul>
<li><a class="reference internal" href="#cudnn-auto-tuner">cuDNN auto-tuner 활성화하기</a></li>
<li><a class="reference internal" href="#cpu-gpu">불필요한 CPU-GPU 동기화 피하기</a></li>
<li><a class="reference internal" href="#tensor">대상 장치에서 직접 tensor 생성하는 방법</a></li>
<li><a class="reference internal" href="#id14">혼합 정밀도와 AMP 사용하는 방법</a></li>
<li><a class="reference internal" href="#id16">가변 입력 길이에 대비하여 메모리 미리 할당하기</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id17">분산 최적화 방법</a><ul>
<li><a class="reference internal" href="#id18">효율적인 데이터 병렬 백엔드 사용하는 방법</a></li>
<li><a class="reference internal" href="#distributeddataparallel-all-reduce">학습할 때 <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> 이나 변화도 축적 사용 시 불필요한 all-reduce 건너뛰는 방법</a></li>
<li><a class="reference internal" href="#distributeddataparallel-find-unused-parameters-true"><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel(find_unused_parameters=True)</span></code> 를 사용할 때 생성자와 실행 레이어 순서를 일치시키는 방법</a></li>
<li><a class="reference internal" href="#id21">분산 설정에서 작업 부하를 분산하는 방법</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  
  
     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/clipboard.min.js"></script>
         <script src="../../_static/copybutton.js"></script>
         <script src="../../_static/translations.js"></script>
         <script src="../../_static/katex.min.js"></script>
         <script src="../../_static/auto-render.min.js"></script>
         <script src="../../_static/katex_autorenderer.js"></script>
         <script src="../../_static/design-tabs.js"></script>
     
  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
//add microsoft link
if(window.location.href.indexOf("/beginner/basics/")!= -1)
{
  var url="https://docs.microsoft.com/learn/paths/pytorch-fundamentals/?wt.mc_id=aiml-7486-cxa";
  switch(window.location.pathname.split("/").pop().replace('.html',''))
  {
    case"quickstart_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/9-quickstart?WT.mc_id=aiml-7486-cxa";
      break;
    case"tensorqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/2-tensors?WT.mc_id=aiml-7486-cxa";
      break;
    case"data_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/3-data?WT.mc_id=aiml-7486-cxa";
      break;
    case"transforms_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/4-transforms?WT.mc_id=aiml-7486-cxa";
      break;
    case"buildmodel_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/5-model?WT.mc_id=aiml-7486-cxa";
      break;
    case"autogradqs_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/6-autograd?WT.mc_id=aiml-7486-cxa";
      break;
    case"optimization_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/7-optimization?WT.mc_id=aiml-7486-cxa";
      break;
    case"saveloadrun_tutorial":
      url="https://docs.microsoft.com/learn/modules/intro-machine-learning-pytorch/8-inference?WT.mc_id=aiml-7486-cxa";
    }

    $(".pytorch-call-to-action-links").children().first().before("<a href="+url+' data-behavior="call-to-action-event" data-response="Run in Microsoft Learn" target="_blank"><div id="microsoft-learn-link" style="padding-bottom: 0.625rem;border-bottom: 1px solid #f3f4f7;padding-right: 2.5rem;display: -webkit-box;  display: -ms-flexbox; display: flex; -webkit-box-align: center;-ms-flex-align: center;align-items: center;"><img class="call-to-action-img" src="../../_static/images/microsoft-logo.svg"/><div class="call-to-action-desktop-view">Run in Microsoft Learn</div><div class="call-to-action-mobile-view">Learn</div></div></a>')
  }
</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-LZRD6GXDLF"></script>
<script data-cfasync="false">
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-LZRD6GXDLF');   // GA4
  gtag('config', 'UA-71919972-3');  // UA
</script>


<script data-cfasync="false">
  $("[data-behavior='call-to-action-event']").on('click', function(){
    ga('send', {
      hitType: 'event',
      eventCategory: $(this).attr("data-response"),
      eventAction: 'click',
      eventLabel: window.location.href
    });

    gtag('event', 'click', {
      'event_category': $(this).attr("data-response"),
      'event_label': $("h1").first().text(),
      'tutorial_link': window.location.href
    });
   });

   $("[data-behavior='tutorial-rating']").on('click', function(){
    gtag('event', 'click', {
      'event_category': 'Tutorial Rating',
      'event_label': $("h1").first().text(),
      'value': $(this).attr("data-count"),
      'customEvent:Rating': $(this).attr("data-count") // send to GA custom dimension customEvent:Rating.
    });
   });

   if (location.pathname == "/") {
     $(".rating-container").hide();
     $(".hr-bottom").hide();
   }
</script>

<script type="text/javascript">
  var collapsedSections = ['파이토치(PyTorch) 레시피', '파이토치(PyTorch) 배우기', 'Introduction to PyTorch on YouTube', '이미지/비디오', '오디오', '텍스트', '백엔드', '강화학습', 'PyTorch 모델을 프로덕션 환경에 배포하기', 'PyTorch 프로파일링', 'Code Transforms with FX', '프론트엔드 API', 'PyTorch 확장하기', '모델 최적화', '병렬 및 분산 학습', 'Edge with ExecuTorch', '추천 시스템', 'Multimodality'];
</script>



  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>PyTorchKorea @ GitHub</h2>
          <p>파이토치 한국 사용자 모임을 GitHub에서 만나보세요.</p>
          <a class="with-right-arrow" href="https://github.com/PyTorchKorea" target="_blank">GitHub로 이동</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>한국어 튜토리얼</h2>
          <p>한국어로 번역 중인 PyTorch 튜토리얼입니다.</p>
          <a class="with-right-arrow" href="https://tutorials.pytorch.kr/">튜토리얼로 이동</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>커뮤니티</h2>
          <p>다른 사용자들과 의견을 나누고, 도와주세요!</p>
          <a class="with-right-arrow" href="https://discuss.pytorch.kr/">커뮤니티로 이동</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.kr/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.kr/">파이토치 한국 사용자 모임</a></li>
            <li><a href="https://pytorch.kr//about">사용자 모임 소개</a></li>
            <li><a href="https://pytorch.kr//about/contributors">기여해주신 분들</a></li>
            <li><a href="https://pytorch.kr//resources">리소스</a></li>
            <li><a href="https://pytorch.kr//coc">행동 강령</a></li>
          </ul>
        </div>
      </div>
      <div class="trademark-disclaimer">
        <ul>
          <li>이 사이트는 독립적인 파이토치 사용자 커뮤니티로, 최신 버전이 아니거나 잘못된 내용이 포함되어 있을 수 있습니다. This site is an independent user community and may be out of date or contain incorrect information.</li>
          <li><a href="https://pytorch.kr/coc">행동 강령</a>을 읽고 지켜주세요. PyTorch 공식 로고 사용 정책은 <a href="https://www.linuxfoundation.org/policies/">Linux Foundation의 정책</a>을 따릅니다. Please read and follow <a href="https://pytorch.kr/coc">our code of conduct</a>. All PyTorch trademark policy applicable to <a href="https://www.linuxfoundation.org/policies/">Linux Foundation's policies</a>.</li>
        </ul>
      </div>
    </div>
  </footer>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.kr/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.kr/get-started">시작하기</a>
          </li>

          <li class="active">
            <a href="https://tutorials.pytorch.kr/">튜토리얼</a>
          </li>

          <li>
            <a href="https://pytorch.kr/hub">허브</a>
          </li>

          <li>
            <a href="https://discuss.pytorch.kr/">커뮤니티</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>